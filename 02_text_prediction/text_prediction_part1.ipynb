{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Load Dataset + Quickstart of Pretrained Backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will load two datasets using the `nlp_data` command and then try out different backbone models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "import gluonnlp\n",
    "from gluonnlp.utils import set_seed\n",
    "mx.npx.set_np()\n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download two datasets from the [GLUE benchmark](https://gluebenchmark.com/):\n",
    "- The Standford Sentiment Treebank (SST-2)\n",
    "- Semantic Textual Similarity Benchmark (STS-B)\n",
    "\n",
    "We will later show how to train prediction models on these two datasets with GluonNLP.\n",
    "\n",
    "First of all, to download the dataset, let's just use the `nlp_data` command. The downloaded dataset are preprocessed to the [parquet](https://parquet.apache.org/) format that can be loaded by [pandas](https://pandas.pydata.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading glue to \"glue\". Selected tasks = sst\n",
      "Processing sst...\n",
      "Found!\n",
      "Downloading glue to \"glue\". Selected tasks = sts\n",
      "Processing sts...\n",
      "Found!\n",
      "sst  sts\n"
     ]
    }
   ],
   "source": [
    "!nlp_data prepare_glue --benchmark glue -t sst\n",
    "!nlp_data prepare_glue --benchmark glue -t sts\n",
    "!ls glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sst_train_df = pd.read_parquet('glue/sst/train.parquet')\n",
    "sst_valid_df = pd.read_parquet('glue/sst/dev.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>that 's far too tragic to merit such superfici...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>demonstrates that the director of such hollywo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>of saucy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a depressed fifteen-year-old 's suicidal poetry</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>are more deeply thought through than in most `...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0       hide new secretions from the parental units       0\n",
       "1               contains no wit , only labored gags       0\n",
       "2  that loves its characters and communicates som...      1\n",
       "3  remains utterly satisfied to remain the same t...      0\n",
       "4  on the worst revenge-of-the-nerds clichés the ...      0\n",
       "5  that 's far too tragic to merit such superfici...      0\n",
       "6  demonstrates that the director of such hollywo...      1\n",
       "7                                          of saucy       1\n",
       "8   a depressed fifteen-year-old 's suicidal poetry       0\n",
       "9  are more deeply thought through than in most `...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_train_df = pd.read_parquet('glue/sts/train.parquet')\n",
    "sts_valid_df = pd.read_parquet('glue/sts/dev.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>genre</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Some men are fighting.</td>\n",
       "      <td>Two men are fighting.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A man is smoking.</td>\n",
       "      <td>A man is skating.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The man is playing the piano.</td>\n",
       "      <td>The man is playing the guitar.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A man is playing on a guitar and singing.</td>\n",
       "      <td>A woman is playing an acoustic guitar and sing...</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A person is throwing a cat on to the ceiling.</td>\n",
       "      <td>A person throws a cat on the ceiling.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sentence1  \\\n",
       "0                         A plane is taking off.   \n",
       "1                A man is playing a large flute.   \n",
       "2  A man is spreading shreded cheese on a pizza.   \n",
       "3                   Three men are playing chess.   \n",
       "4                    A man is playing the cello.   \n",
       "5                         Some men are fighting.   \n",
       "6                              A man is smoking.   \n",
       "7                  The man is playing the piano.   \n",
       "8      A man is playing on a guitar and singing.   \n",
       "9  A person is throwing a cat on to the ceiling.   \n",
       "\n",
       "                                           sentence2          genre  score  \n",
       "0                        An air plane is taking off.  main-captions   5.00  \n",
       "1                          A man is playing a flute.  main-captions   3.80  \n",
       "2  A man is spreading shredded cheese on an uncoo...  main-captions   3.80  \n",
       "3                         Two men are playing chess.  main-captions   2.60  \n",
       "4                 A man seated is playing the cello.  main-captions   4.25  \n",
       "5                              Two men are fighting.  main-captions   4.25  \n",
       "6                                  A man is skating.  main-captions   0.50  \n",
       "7                     The man is playing the guitar.  main-captions   1.60  \n",
       "8  A woman is playing an acoustic guitar and sing...  main-captions   2.20  \n",
       "9              A person throws a cat on the ceiling.  main-captions   5.00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart of Pretrained Backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bunch of recent papers, especially [BERT](https://arxiv.org/pdf/1810.04805.pdf), have led a new trend for solving NLP problems:\n",
    "- Pretrain a backbone model on a large corpus,\n",
    "- Finetune the backbone to solve the specific NLP task.\n",
    "\n",
    "GluonNLP provides the interface for using the pretrained backbone models. Here, for quickstart, let's load the BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the backbone models via `get_backbone`. For example, you can run the following command to get the backbone of `google_en_cased_bert_base`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonnlp.models import get_backbone\n",
    "model_name = 'google_en_cased_bert_base'\n",
    "model_cls, cfg, tokenizer, local_params_path, _ = get_backbone(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Model Class:\n",
      "<class 'gluonnlp.models.bert.BertModel'>\n",
      "\n",
      "- Configuration:\n",
      "INITIALIZER:\n",
      "  bias: ['zeros']\n",
      "  embed: ['truncnorm', 0, 0.02]\n",
      "  weight: ['truncnorm', 0, 0.02]\n",
      "MODEL:\n",
      "  activation: gelu\n",
      "  attention_dropout_prob: 0.1\n",
      "  compute_layout: auto\n",
      "  dtype: float32\n",
      "  hidden_dropout_prob: 0.1\n",
      "  hidden_size: 3072\n",
      "  layer_norm_eps: 1e-12\n",
      "  layout: NT\n",
      "  max_length: 512\n",
      "  num_heads: 12\n",
      "  num_layers: 12\n",
      "  num_token_types: 2\n",
      "  pos_embed_type: learned\n",
      "  units: 768\n",
      "  vocab_size: 28996\n",
      "VERSION: 1\n",
      "\n",
      "- Tokenizer:\n",
      "HuggingFaceWordPieceTokenizer(\n",
      "   vocab_file = /home/ubuntu/.mxnet/models/nlp/google_en_cased_bert_base/vocab-c1defaaa.json\n",
      "   unk_token = [UNK], sep_token = [SEP], cls_token = [CLS]\n",
      "   pad_token = [PAD], mask_token = [MASK]\n",
      "   clean_text = True, handle_chinese_chars = True\n",
      "   strip_accents = False, lowercase = False\n",
      "   wordpieces_prefix = ##\n",
      "   vocab = Vocab(size=28996, unk_token=\"[UNK]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\", sep_token=\"[SEP]\", mask_token=\"[MASK]\")\n",
      ")\n",
      "\n",
      "- Path of the weights:\n",
      "/home/ubuntu/.mxnet/models/nlp/google_en_cased_bert_base/model-c566c289.params\n"
     ]
    }
   ],
   "source": [
    "print('- Model Class:')\n",
    "print(model_cls)\n",
    "print('\\n- Configuration:')\n",
    "print(cfg)\n",
    "print('\\n- Tokenizer:')\n",
    "print(tokenizer)\n",
    "print('\\n- Path of the weights:')\n",
    "print(local_params_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Backbone\n",
    "\n",
    "To create a new backbone model in Gluon, you can just use the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (encoder): BertTransformer(\n",
      "    (all_layers): HybridSequential(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (6): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (7): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (8): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (9): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (10): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (11): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_embed): Embedding(28996 -> 768, float32)\n",
      "  (embed_layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "  (embed_dropout): Dropout(p = 0.1, axes=())\n",
      "  (token_type_embed): Embedding(2 -> 768, float32)\n",
      "  (token_pos_embed): PositionalEmbedding(\n",
      "    (_embed): LearnedPositionalEmbedding(units=768, max_length=512, mode=clip, dtype=float32)\n",
      "  )\n",
      "  (pooler): Dense(768 -> 768, Activation(tanh))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "backbone = model_cls.from_cfg(cfg)\n",
    "backbone.hybridize()\n",
    "backbone.load_parameters(local_params_path)\n",
    "print(backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly use the `backbone` to extract the embeddings. For BERT, it will output one embedding vector for the whole sentence --- `cls_embedding` and a bounch of contextual embedding vectors for each token --- `token_embeddings`.\n",
    "\n",
    "[INSERT FIGURE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence= hide new secretions from the parental units \n",
      "Tokens= ['hide', 'new', 'secret', '##ions', 'from', 'the', 'parental', 'units']\n",
      "Token IDs= [[  101.  4750.  1207.  3318.  5266.  1121.  1103. 22467.  2338.   102.]]\n",
      "Token Types= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Valid Length= [10.]\n"
     ]
    }
   ],
   "source": [
    "text_input = sst_train_df['sentence'][0]\n",
    "tokens = tokenizer.encode(text_input, str)\n",
    "token_ids = tokenizer.encode(text_input, int)\n",
    "token_ids = mx.np.array([[tokenizer.vocab.cls_id] + token_ids + [tokenizer.vocab.sep_id]])\n",
    "token_types = mx.np.array([0] * len(token_ids[0]))\n",
    "valid_length = mx.np.array([len(token_ids[0])])\n",
    "print('Sentence=', text_input)\n",
    "print('Tokens=', tokens)\n",
    "print('Token IDs=', token_ids)\n",
    "print('Token Types=', token_types)\n",
    "print('Valid Length=', valid_length)\n",
    "token_embeddings, cls_embedding = backbone(token_ids, token_types, valid_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n",
      "[[-0.7091232   0.5309977   0.9998003  -0.9866606   0.9401921   0.6315886\n",
      "   0.9837744  -0.96095276 -0.95775586 -0.60642767  0.97397816  0.99566174\n",
      "  -0.9902677  -0.99969244  0.435707   -0.96778107  0.97976995 -0.52170444\n",
      "  -0.99992543 -0.38844424 -0.29302844 -0.9997452   0.28357023  0.89527607\n",
      "   0.973925    0.09718102  0.9790452   0.9999052   0.8216678  -0.08883981\n",
      "   0.26693714 -0.98289573  0.6802321  -0.99808586  0.22781204  0.10108362\n",
      "   0.24586591 -0.27913764  0.5857332  -0.8314479  -0.571512   -0.23013882\n",
      "   0.4970959  -0.45805666  0.65454197  0.3267932   0.17924476 -0.04300154\n",
      "  -0.19947378  0.99990696 -0.95206505  0.99967813 -0.96945816  0.99622405\n",
      "   0.98914737  0.46140316  0.9910741   0.15409192 -0.992088    0.3981389\n",
      "   0.95724404  0.10991149  0.8809637  -0.15570553  0.33269107 -0.47607294\n",
      "  -0.6478572   0.24575822 -0.52706516  0.32258353  0.4838301   0.22356257\n",
      "   0.96965253 -0.8545226  -0.02529192 -0.8779966   0.08907609 -0.99979186\n",
      "   0.9317021   0.99988896  0.49601823 -0.9991174   0.99054784 -0.3568779\n",
      "  -0.22942463  0.02595962 -0.9879694  -0.99868053  0.13015969 -0.69756526\n",
      "   0.6479567  -0.97974074  0.48520502 -0.64439225  0.9999195  -0.8339863\n",
      "  -0.16350758  0.34621865  0.78150666 -0.3555579  -0.62563574  0.8185205\n",
      "   0.9861365  -0.95692414  0.99028486  0.17341003 -0.92722744 -0.56023645\n",
      "   0.20557603  0.13732557  0.9856703  -0.9890133  -0.73785186  0.12957247\n",
      "   0.8658399  -0.80035186  0.9805146   0.71842194 -0.35788894  0.9999255\n",
      "  -0.13492677  0.80868644  0.9962814   0.41495082 -0.30475485 -0.23063129\n",
      "  -0.48100984  0.73613644 -0.3244665  -0.33410993  0.7040149  -0.98624015\n",
      "  -0.98390603  0.9989357  -0.29645607  0.99992025 -0.9983025   0.96804315\n",
      "  -0.99987584 -0.7954264  -0.55092233  0.00173598 -0.9006552   0.33315492\n",
      "   0.98617136  0.06268346 -0.6775684  -0.5006535   0.36791107 -0.4613807\n",
      "   0.60644454  0.68072975 -0.9544994   0.9988955   0.9858681   0.8297699\n",
      "   0.90326345  0.1764953  -0.7843319   0.71048725  0.9793454  -0.998678\n",
      "   0.6898537  -0.97399235  0.99894214  0.93739164  0.49125305 -0.9814041\n",
      "   0.99976176 -0.13819456 -0.00881221 -0.18429133 -0.13458963 -0.98921126\n",
      "   0.4311682   0.43057305  0.80257094  0.9995556  -0.98920584  0.9997047\n",
      "   0.99279153  0.18337989  0.72448045  0.99094504 -0.9918539  -0.97047716\n",
      "  -0.97837234  0.34525993  0.07042124  0.5316874   0.2812754   0.94442517\n",
      "   0.9848955   0.5563385  -0.9978581  -0.32191846  0.97047484 -0.0683724\n",
      "   0.9999064  -0.29651383 -0.9995534  -0.6966984   0.6940961   0.9551153\n",
      "  -0.22836089  0.9517472  -0.4574069  -0.1655601   0.9410036  -0.9993509\n",
      "   0.96731234  0.23766254  0.35262743  0.81705934  0.9906867  -0.42077166\n",
      "  -0.19564216  0.21587051 -0.57465816  0.9996771  -0.99902815 -0.2666343\n",
      "   0.28968245 -0.9902607  -0.9945611   0.98201895  0.034636   -0.721371\n",
      "  -0.2549365  -0.18457262  0.31309575  0.69343877  0.978191   -0.20347977\n",
      "  -0.00481401 -0.9995792  -0.9663464  -0.7106698  -0.8586005   0.1054161\n",
      "   0.6408008  -0.40886474 -0.88061756 -0.9806821   0.95261985  0.76261485\n",
      "  -0.6342651  -0.18440774 -0.3107097  -0.97628516  0.19034064 -0.5027426\n",
      "  -0.99835557  0.9991186  -0.5748996   0.9768463   0.9473164  -0.9920084\n",
      "   0.5044286  -0.985901   -0.17023107 -0.9990103   0.09609929  0.15818793\n",
      "  -0.31264448  0.0106758   0.98402965 -0.9690242  -0.5931927   0.36156183\n",
      "  -0.9997676   0.8825949  -0.11184862  0.9977687   0.02751607 -0.0865851\n",
      "   0.97613406  0.7962426  -0.9820053  -0.9995618   0.6427753   0.9974565\n",
      "  -0.9924495  -0.21130863  0.9997925  -0.9741626  -0.7487999  -0.94033426\n",
      "  -0.9906049  -0.9992529  -0.04490396 -0.54169667  0.2268141   0.9785514\n",
      "   0.29549214  0.0284842   0.99035627  0.99461377  0.13285175 -0.09193343\n",
      "   0.0949246  -0.9673264  -0.9976206   0.40934548  0.18352103 -0.9999129\n",
      "   0.9996518  -0.989367    0.99935895  0.76923645 -0.97501844  0.7520884\n",
      "   0.09167927 -0.8396409  -0.02403094  0.99973977  0.97766805 -0.08770724\n",
      "   0.09471986  0.64937276 -0.15149954  0.30610043 -0.28071812 -0.25184026\n",
      "   0.24202141 -0.92977554  0.94696516  0.28097022 -0.9847736   0.9747086\n",
      "   0.04741835  0.6460993  -0.39200076  0.85491765  0.9904884  -0.06974538\n",
      "  -0.15210417 -0.25004867 -0.97438926 -0.9099685   0.15399939 -0.98691726\n",
      "  -0.26526964  0.8275368   0.97229755 -0.9772698   0.9964046  -0.23485038\n",
      "   0.65983975 -0.9802499   0.9999465  -0.9973961   0.0839857   0.4154574\n",
      "  -0.5395337   0.02196989  0.9850348   0.93871695  0.9160993  -0.7096048\n",
      "  -0.23356444  0.8039957   0.9424728  -0.9024318  -0.11842214 -0.98959625\n",
      "  -0.03392011  0.99405116  0.98628634 -0.08400201 -0.37537113 -0.98401654\n",
      "   0.96780795 -0.54461026 -0.5032569  -0.08069275 -0.69519067  0.5669698\n",
      "   0.98094887 -0.27594608  0.5714332   0.16811155 -0.9841504   0.609805\n",
      "   0.6545892   0.999449   -0.96163726  0.41721115  0.97997844 -0.20373635\n",
      "  -0.6512965   0.4840875   0.9912593  -0.920004   -0.28643855 -0.9990092\n",
      "  -0.1143152  -0.6892625  -0.01464462 -0.24958628  0.05449535 -0.6673372\n",
      "   0.8809227  -0.17344108  0.7497996  -0.10982293  0.95716614 -0.18646939\n",
      "  -0.12268528 -0.38323078 -0.13355835  0.43013167  0.15074706  0.97869563\n",
      "  -0.957543    0.99980396 -0.04298814 -0.99986565 -0.97093475 -0.5304731\n",
      "  -0.9996938   0.15482447 -0.99585706  0.98242986  0.7551313  -0.9753379\n",
      "  -0.9914058  -0.9972046  -0.99868554  0.40280738  0.51788986 -0.02767387\n",
      "   0.42737168  0.9570443   0.04388629 -0.029463   -0.19246452 -0.9312641\n",
      "  -0.5006752  -0.9787913   0.4917452  -0.99991494 -0.36954612  0.9908675\n",
      "  -0.9833681  -0.6387664  -0.885851   -0.61867034 -0.83716226  0.4951994\n",
      "   0.9788494  -0.04468467  0.02384792 -0.99964607  0.98735285 -0.5894184\n",
      "   0.15612549 -0.65928215 -0.9664059   0.99946576  0.73891807 -0.11791474\n",
      "  -0.18034023 -0.99858963  0.9240635  -0.7323336  -0.7881336  -0.97647244\n",
      "   0.1279202  -0.9279718  -0.99970496 -0.03300219  0.96386474  0.9964713\n",
      "   0.96733636  0.38594526 -0.35144287 -0.9588593   0.1878367  -0.9998838\n",
      "   0.5125011   0.6969539  -0.97450703 -0.6020313   0.9911549   0.9654626\n",
      "  -0.6311809  -0.92731655  0.62024844  0.37682667  0.9708397  -0.2454063\n",
      "  -0.4695728   0.28620476 -0.12361199 -0.9843397  -0.92410815  0.99113506\n",
      "  -0.986546    0.9797539   0.96502906  0.98822355 -0.12194871  0.04152082\n",
      "  -0.89579296 -0.9975606  -0.6479943   0.2832552  -0.9998468   0.99986184\n",
      "  -0.99989796  0.4510291  -0.20131288  0.7551497   0.98238605 -0.27854282\n",
      "  -0.99977267 -0.9995714   0.11068585 -0.03108905  0.9866784   0.1842067\n",
      "   0.16080104 -0.35277152  0.1351859   0.99359876 -0.55838424 -0.17116955\n",
      "  -0.9720952   0.9992649   0.69940495 -0.9960012   0.9665609  -0.9991914\n",
      "   0.7582753   0.9617448   0.84028673  0.971999   -0.98600656  0.99990547\n",
      "  -0.9997796   0.99781585 -0.99992436 -0.9904515   0.9995296  -0.9891726\n",
      "  -0.20931248 -0.9994523  -0.98277146  0.372486    0.1802355  -0.50284994\n",
      "   0.98196745 -0.99973005 -0.9966526   0.12603398 -0.71759033 -0.44829512\n",
      "   0.9808356  -0.33139265  0.98844314 -0.06259328  0.95060116  0.03949001\n",
      "   0.984584    0.99862087 -0.6350018  -0.62171257 -0.9838822   0.9807421\n",
      "  -0.19780138  0.31510144  0.9466682  -0.02371383 -0.70718753  0.42388788\n",
      "  -0.9940535   0.26364648 -0.78225756  0.8736045   0.61190075  0.82043064\n",
      "  -0.06348383 -0.49339753 -0.17983928 -0.98904085  0.4498496  -0.99920094\n",
      "   0.9804476  -0.7618978  -0.0021842  -0.38139018  0.3289212  -0.94438195\n",
      "   0.99928355  0.9957442  -0.99952483  0.13811076  0.9766586  -0.59896296\n",
      "   0.9626906  -0.986763   -0.0120844   0.8816149  -0.6924968   0.9698316\n",
      "   0.24679704 -0.09842137  0.97499305 -0.98949784 -0.63123775 -0.62095857\n",
      "   0.29053485  0.16998416 -0.95247966  0.19300872  0.9220837  -0.240688\n",
      "  -0.99933314  0.5026562  -0.9985959  -0.10911288  0.9524685  -0.02372052\n",
      "   0.99975544 -0.57190335  0.01308771  0.09345745 -0.9993108  -0.99177545\n",
      "   0.1419109  -0.18305089 -0.77246916  0.98824185 -0.13539691  0.34679505\n",
      "  -0.9998639   0.33893275  0.9720028   0.302726    0.79635143 -0.21586746\n",
      "  -0.9564001  -0.836192   -0.5867136  -0.00189628  0.7351253  -0.9377271\n",
      "  -0.2921344  -0.58455086  0.9999166  -0.9929377  -0.9115775  -0.988452\n",
      "   0.11219229  0.63725793  0.4501295   0.12361388 -0.6843909   0.7603837\n",
      "  -0.8302419   0.99136376 -0.9862675  -0.9926844   0.99942565  0.48086467\n",
      "  -0.96751815 -0.12368567 -0.24969056  0.18269457  0.0272303   0.584342\n",
      "  -0.7905712  -0.15854189 -0.9981454   0.4751401  -0.4371174  -0.98824376\n",
      "  -0.48161128 -0.30607948 -0.9994892   0.9857387   0.95515406  0.9998242\n",
      "  -0.9993335   0.7807244   0.13713294  0.9982674   0.05805499 -0.6261462\n",
      "   0.6714307   0.9994523  -0.29264215  0.553848   -0.05584519  0.06547522\n",
      "   0.07479848 -0.45587334  0.98433244 -0.78209186  0.18247312 -0.9704198\n",
      "  -0.9998233   0.9998738  -0.00530515  0.98284584  0.3846703   0.6727673\n",
      "  -0.8167004   0.89701533 -0.90385157 -0.89194626 -0.99992365  0.41347933\n",
      "  -0.9995891  -0.9780134   0.08102933  0.98007154 -0.99915904 -0.9903672\n",
      "  -0.2980201  -0.99992     0.68037724 -0.9541764  -0.39615244 -0.9796596\n",
      "   0.9813396  -0.34680733  0.13625708  0.96203184 -0.95929444  0.835272\n",
      "   0.7108792   0.7989696   0.28953674  0.18872742 -0.56163335 -0.9718882\n",
      "  -0.6706432  -0.9727423   0.41449523 -0.96833426 -0.61905247  0.9942177\n",
      "   0.9778929  -0.9990125  -0.9910463   0.97953755  0.12185977  0.98464835\n",
      "  -0.52560043 -0.9996094  -0.9996547   0.30278003 -0.2441911   0.98883814\n",
      "  -0.37476188  0.99898016  0.7834062  -0.26135242  0.5787804  -0.19950174\n",
      "  -0.32907224 -0.25714558 -0.20735504  0.9999148  -0.65752685  0.9759005 ]]\n"
     ]
    }
   ],
   "source": [
    "print(cls_embedding.shape)\n",
    "print(cls_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 768)\n",
      "[[[ 0.25098458 -0.20093301 -0.01786663 ... -0.35123444  0.4021042\n",
      "   -0.16942185]\n",
      "  [ 0.28456497 -0.59436226 -0.07495949 ...  0.36519405  0.4808873\n",
      "    0.28281727]\n",
      "  [ 0.09450649 -0.06759122  0.40153563 ...  0.36970523  0.4600461\n",
      "    0.04629947]\n",
      "  ...\n",
      "  [ 0.19983977 -0.17685743  0.09971484 ... -0.09942135  0.36892515\n",
      "    0.28015777]\n",
      "  [ 0.11791191 -0.18985073 -0.02815771 ... -0.22512732 -0.14774892\n",
      "   -0.0953014 ]\n",
      "  [ 0.33991298 -0.03747199 -0.13523774 ... -0.9031357   1.1424115\n",
      "   -0.5947777 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(token_embeddings.shape)\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Backbone Models in GluonNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from BERT, GluonNLP has provided other backbone models including the recent models like [XLMR](https://arxiv.org/pdf/1911.02116.pdf), [ALBERT](https://arxiv.org/pdf/1909.11942.pdf), [ELECTRA](https://openreview.net/pdf?id=r1xMH1BtvB), and [MobileBERT](https://arxiv.org/pdf/2004.02984.pdf). We can use `list_backbone_names` to list all the backbones that are supported in GluonNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['google_albert_base_v2',\n",
       " 'google_albert_large_v2',\n",
       " 'google_albert_xlarge_v2',\n",
       " 'google_albert_xxlarge_v2',\n",
       " 'google_en_cased_bert_base',\n",
       " 'google_en_cased_bert_large',\n",
       " 'google_en_cased_bert_wwm_large',\n",
       " 'google_en_uncased_bert_base',\n",
       " 'google_en_uncased_bert_large',\n",
       " 'google_en_uncased_bert_wwm_large',\n",
       " 'google_multi_cased_bert_base',\n",
       " 'google_zh_bert_base',\n",
       " 'gluon_electra_small_owt',\n",
       " 'google_electra_base',\n",
       " 'google_electra_large',\n",
       " 'google_electra_small',\n",
       " 'gpt2_124M',\n",
       " 'gpt2_355M',\n",
       " 'gpt2_774M',\n",
       " 'google_uncased_mobilebert',\n",
       " 'fairseq_roberta_base',\n",
       " 'fairseq_roberta_large',\n",
       " 'fairseq_xlmr_base',\n",
       " 'fairseq_xlmr_large',\n",
       " 'fairseq_bart_base',\n",
       " 'fairseq_bart_large']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gluonnlp.models import list_backbone_names\n",
    "list_backbone_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of the command, we can compare the number of params of some chosen backbone models:\n",
    "- google_en_uncased_bert_base\n",
    "- google_albert_base_v2\n",
    "- google_uncased_mobilebert\n",
    "- fairseq_bart_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/mxnet/gluon/block.py:571: UserWarning: Parameter 'weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>#Params (MB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google_en_uncased_bert_base</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google_albert_base_v2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google_uncased_mobilebert</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fairseq_bart_base</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  #Params (MB)\n",
       "0  google_en_uncased_bert_base           109\n",
       "1        google_albert_base_v2            11\n",
       "2    google_uncased_mobilebert            24\n",
       "3            fairseq_bart_base           218"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gluonnlp.utils.misc import count_parameters\n",
    "param_num_l = []\n",
    "for name in ['google_en_uncased_bert_base',\n",
    "             'google_albert_base_v2',\n",
    "             'google_uncased_mobilebert',\n",
    "             'fairseq_bart_base']:\n",
    "    model_cls, cfg, tokenizer, local_params_path, _ = get_backbone(name, load_backbone=False)\n",
    "    model = model_cls.from_cfg(cfg)\n",
    "    model.hybridize()\n",
    "    model.initialize()\n",
    "    total_num_params, fixed_num_params = count_parameters(model.collect_params())\n",
    "    mx.npx.waitall()\n",
    "    param_num_l.append((name, int(total_num_params / 1000000)))\n",
    "df = pd.DataFrame(param_num_l, columns=['Model', '#Params (MB)'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Other Backbones\n",
    "\n",
    "Apart from BERT, it's straightforward to load other backbone models.\n",
    "#### Load ALBERT-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZER:\n",
      "  bias: ['zeros']\n",
      "  embed: ['truncnorm', 0, 0.02]\n",
      "  weight: ['truncnorm', 0, 0.02]\n",
      "MODEL:\n",
      "  activation: gelu(tanh)\n",
      "  attention_dropout_prob: 0.0\n",
      "  compute_layout: auto\n",
      "  dtype: float32\n",
      "  embed_size: 128\n",
      "  hidden_dropout_prob: 0.0\n",
      "  hidden_size: 3072\n",
      "  layer_norm_eps: 1e-12\n",
      "  layout: NT\n",
      "  max_length: 512\n",
      "  num_groups: 1\n",
      "  num_heads: 12\n",
      "  num_layers: 12\n",
      "  num_token_types: 2\n",
      "  pos_embed_type: learned\n",
      "  units: 768\n",
      "  vocab_size: 30000\n",
      "VERSION: 1\n",
      "\n",
      "SentencepieceTokenizer(\n",
      "   model_path = /home/ubuntu/.mxnet/models/nlp/google_albert_base_v2/spm-65999e5d.model\n",
      "   lowercase = True, nbest = 0, alpha = 0.0\n",
      "   vocab = Vocab(size=30000, unk_token=\"<unk>\", pad_token=\"<pad>\", cls_token=\"[CLS]\", sep_token=\"[SEP]\", mask_token=\"[MASK]\")\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_cls, cfg, tokenizer, local_params_path, _ = get_backbone('google_albert_base_v2')\n",
    "backbone = model_cls.from_cfg(cfg)\n",
    "backbone.load_parameters(local_params_path)\n",
    "backbone.hybridize()\n",
    "print(cfg)\n",
    "print()\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MobileBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZER:\n",
      "  bias: ['zeros']\n",
      "  embed: ['truncnorm', 0, 0.02]\n",
      "  weight: ['truncnorm', 0, 0.02]\n",
      "MODEL:\n",
      "  activation: relu\n",
      "  attention_dropout_prob: 0.1\n",
      "  bottleneck_strategy: qk_sharing\n",
      "  classifier_activation: False\n",
      "  compute_layout: auto\n",
      "  dtype: float32\n",
      "  embed_size: 128\n",
      "  hidden_dropout_prob: 0.0\n",
      "  hidden_size: 512\n",
      "  inner_size: 128\n",
      "  layer_norm_eps: 1e-12\n",
      "  layout: NT\n",
      "  max_length: 512\n",
      "  normalization: no_norm\n",
      "  num_heads: 4\n",
      "  num_layers: 24\n",
      "  num_stacked_ffn: 4\n",
      "  num_token_types: 2\n",
      "  pos_embed_type: learned\n",
      "  trigram_embed: True\n",
      "  units: 512\n",
      "  use_bottleneck: True\n",
      "  vocab_size: 30522\n",
      "VERSION: 1\n",
      "\n",
      "HuggingFaceWordPieceTokenizer(\n",
      "   vocab_file = /home/ubuntu/.mxnet/models/nlp/google_uncased_mobilebert/vocab-e6d2b21d.json\n",
      "   unk_token = [UNK], sep_token = [SEP], cls_token = [CLS]\n",
      "   pad_token = [PAD], mask_token = [MASK]\n",
      "   clean_text = True, handle_chinese_chars = True\n",
      "   strip_accents = False, lowercase = True\n",
      "   wordpieces_prefix = ##\n",
      "   vocab = Vocab(size=30522, unk_token=\"[UNK]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\", sep_token=\"[SEP]\", mask_token=\"[MASK]\")\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_cls, cfg, tokenizer, local_params_path, _ = get_backbone('google_uncased_mobilebert')\n",
    "backbone = model_cls.from_cfg(cfg)\n",
    "backbone.load_parameters(local_params_path)\n",
    "backbone.hybridize()\n",
    "print(cfg)\n",
    "print()\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZER:\n",
      "  bias: ['zeros']\n",
      "  embed: ['xavier', 'gaussian', 'in', 1.0]\n",
      "  weight: ['xavier', 'uniform', 'avg', 1.0]\n",
      "MODEL:\n",
      "  DECODER:\n",
      "    activation: gelu\n",
      "    hidden_size: 3072\n",
      "    num_heads: 12\n",
      "    num_layers: 6\n",
      "    pre_norm: False\n",
      "    recurrent: False\n",
      "    units: 768\n",
      "    use_qkv_bias: True\n",
      "  ENCODER:\n",
      "    activation: gelu\n",
      "    hidden_size: 3072\n",
      "    num_heads: 12\n",
      "    num_layers: 6\n",
      "    pre_norm: False\n",
      "    recurrent: False\n",
      "    units: 768\n",
      "    use_qkv_bias: True\n",
      "  activation_dropout: 0.0\n",
      "  attention_dropout: 0.1\n",
      "  data_norm: True\n",
      "  dropout: 0.1\n",
      "  dtype: float32\n",
      "  layer_norm_eps: 1e-05\n",
      "  layout: NT\n",
      "  max_src_length: 1024\n",
      "  max_tgt_length: 1024\n",
      "  pooler_activation: tanh\n",
      "  pos_embed_type: learned\n",
      "  scale_embed: False\n",
      "  shared_embed: True\n",
      "  tie_weights: True\n",
      "  vocab_size: 51201\n",
      "VERSION: 1\n",
      "\n",
      "HuggingFaceByteBPETokenizer(\n",
      "   merges_file = /home/ubuntu/.mxnet/models/nlp/fairseq_bart_base/gpt2-396d4d8e.merges\n",
      "   vocab_file = /home/ubuntu/.mxnet/models/nlp/fairseq_bart_base/gpt2-f4dedacb.vocab\n",
      "   add_prefix_space = False, lowercase = False, dropout = None\n",
      "   unicode_normalizer = None, continuing_subword_prefix = None\n",
      "   end_of_word_suffix = None\n",
      "   trim_offsets = False\n",
      "   vocab = Vocab(size=51201, unk_token=\"<unk>\", bos_token=\"<s>\", pad_token=\"<pad>\", eos_token=\"</s>\", mask_token=\"<mask>\")\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_cls, cfg, tokenizer, local_params_path, _ = get_backbone('fairseq_bart_base')\n",
    "backbone = model_cls.from_cfg(cfg)\n",
    "backbone.load_parameters(local_params_path)\n",
    "backbone.hybridize()\n",
    "print(cfg)\n",
    "print()\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence= hide new secretions from the parental units \n",
      "Tokens= ['hide', 'Ġnew', 'Ġsecret', 'ions', 'Ġfrom', 'Ġthe', 'Ġparental', 'Ġunits', 'Ġ']\n",
      "[11.]\n",
      "(1, 11, 51201)\n"
     ]
    }
   ],
   "source": [
    "text_input = sst_train_df['sentence'][0]\n",
    "tokens = tokenizer.encode(text_input, str)\n",
    "token_ids = tokenizer.encode(text_input, int)\n",
    "token_ids = mx.np.array([[tokenizer.vocab.bos_id] + token_ids + [tokenizer.vocab.eos_id]])\n",
    "valid_length = mx.np.array([len(token_ids[0])])\n",
    "token_embeddings = backbone(token_ids, valid_length, token_ids, valid_length)\n",
    "print('Sentence=', text_input)\n",
    "print('Tokens=', tokens)\n",
    "print(valid_length)\n",
    "print(token_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
