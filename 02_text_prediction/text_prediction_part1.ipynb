{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Load Dataset + Quickstart of Pretrained Backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "import gluonnlp\n",
    "from gluonnlp.utils import set_seed\n",
    "mx.npx.set_np()\n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download two datasets from the [GLUE benchmark](https://gluebenchmark.com/):\n",
    "- The Standford Sentiment Treebank (SST-2)\n",
    "- Semantic Textual Similarity Benchmark (STS-B)\n",
    "\n",
    "We will use these two throughout the tutorial.\n",
    "\n",
    "To download the dataset, we will simply use the `nlp_data` command. The downloaded dataset are preprocessed to the [parquet](https://parquet.apache.org/) format that can be loaded by [pandas](https://pandas.pydata.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading glue to \"glue\". Selected tasks = sst\n",
      "Processing sst...\n",
      "Found!\n",
      "Downloading glue to \"glue\". Selected tasks = sts\n",
      "Processing sts...\n",
      "Found!\n",
      "sst  sts\n"
     ]
    }
   ],
   "source": [
    "!nlp_data prepare_glue --benchmark glue -t sst\n",
    "!nlp_data prepare_glue --benchmark glue -t sts\n",
    "!ls glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet('glue/sst/train.parquet')\n",
    "valid_df = pd.read_parquet('glue/sst/dev.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>that 's far too tragic to merit such superfici...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>demonstrates that the director of such hollywo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>of saucy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a depressed fifteen-year-old 's suicidal poetry</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>are more deeply thought through than in most `...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0       hide new secretions from the parental units       0\n",
       "1               contains no wit , only labored gags       0\n",
       "2  that loves its characters and communicates som...      1\n",
       "3  remains utterly satisfied to remain the same t...      0\n",
       "4  on the worst revenge-of-the-nerds clichés the ...      0\n",
       "5  that 's far too tragic to merit such superfici...      0\n",
       "6  demonstrates that the director of such hollywo...      1\n",
       "7                                          of saucy       1\n",
       "8   a depressed fifteen-year-old 's suicidal poetry       0\n",
       "9  are more deeply thought through than in most `...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet('glue/sts/train.parquet')\n",
    "valid_df = pd.read_parquet('glue/sts/dev.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>genre</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Some men are fighting.</td>\n",
       "      <td>Two men are fighting.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A man is smoking.</td>\n",
       "      <td>A man is skating.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The man is playing the piano.</td>\n",
       "      <td>The man is playing the guitar.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A man is playing on a guitar and singing.</td>\n",
       "      <td>A woman is playing an acoustic guitar and sing...</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A person is throwing a cat on to the ceiling.</td>\n",
       "      <td>A person throws a cat on the ceiling.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sentence1  \\\n",
       "0                         A plane is taking off.   \n",
       "1                A man is playing a large flute.   \n",
       "2  A man is spreading shreded cheese on a pizza.   \n",
       "3                   Three men are playing chess.   \n",
       "4                    A man is playing the cello.   \n",
       "5                         Some men are fighting.   \n",
       "6                              A man is smoking.   \n",
       "7                  The man is playing the piano.   \n",
       "8      A man is playing on a guitar and singing.   \n",
       "9  A person is throwing a cat on to the ceiling.   \n",
       "\n",
       "                                           sentence2          genre  score  \n",
       "0                        An air plane is taking off.  main-captions   5.00  \n",
       "1                          A man is playing a flute.  main-captions   3.80  \n",
       "2  A man is spreading shredded cheese on an uncoo...  main-captions   3.80  \n",
       "3                         Two men are playing chess.  main-captions   2.60  \n",
       "4                 A man seated is playing the cello.  main-captions   4.25  \n",
       "5                              Two men are fighting.  main-captions   4.25  \n",
       "6                                  A man is skating.  main-captions   0.50  \n",
       "7                     The man is playing the guitar.  main-captions   1.60  \n",
       "8  A woman is playing an acoustic guitar and sing...  main-captions   2.20  \n",
       "9              A person throws a cat on the ceiling.  main-captions   5.00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart of Pretrained Backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bunch of recent papers, especially [BERT](https://arxiv.org/pdf/1810.04805.pdf), have led a new trend for solving NLP problems: 1) pretrain a backbone model on a large corpus, 2) finetune the backbone on a specific NLP task.\n",
    "\n",
    "GluonNLP provides the interface for accessing to the pretrained backbone models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonnlp.models import get_backbone\n",
    "model_name = 'google_en_cased_bert_base'\n",
    "model_cls, cfg, tokenizer, local_params_path, _ = get_backbone(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Model Class:\n",
      "<class 'gluonnlp.models.bert.BertModel'>\n",
      "\n",
      "- Configuration:\n",
      "INITIALIZER:\n",
      "  bias: ['zeros']\n",
      "  embed: ['truncnorm', 0, 0.02]\n",
      "  weight: ['truncnorm', 0, 0.02]\n",
      "MODEL:\n",
      "  activation: gelu\n",
      "  attention_dropout_prob: 0.1\n",
      "  compute_layout: auto\n",
      "  dtype: float32\n",
      "  hidden_dropout_prob: 0.1\n",
      "  hidden_size: 3072\n",
      "  layer_norm_eps: 1e-12\n",
      "  layout: NT\n",
      "  max_length: 512\n",
      "  num_heads: 12\n",
      "  num_layers: 12\n",
      "  num_token_types: 2\n",
      "  pos_embed_type: learned\n",
      "  units: 768\n",
      "  vocab_size: 28996\n",
      "VERSION: 1\n",
      "\n",
      "- Tokenizer:\n",
      "HuggingFaceWordPieceTokenizer(\n",
      "   vocab_file = /home/ubuntu/.mxnet/models/nlp/google_en_cased_bert_base/vocab-c1defaaa.json\n",
      "   unk_token = [UNK], sep_token = [SEP], cls_token = [CLS]\n",
      "   pad_token = [PAD], mask_token = [MASK]\n",
      "   clean_text = True, handle_chinese_chars = True\n",
      "   strip_accents = False, lowercase = False\n",
      "   wordpieces_prefix = ##\n",
      "   vocab = Vocab(size=28996, unk_token=\"[UNK]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\", sep_token=\"[SEP]\", mask_token=\"[MASK]\")\n",
      ")\n",
      "\n",
      "- Path of the weights:\n",
      "/home/ubuntu/.mxnet/models/nlp/google_en_cased_bert_base/model-c566c289.params\n"
     ]
    }
   ],
   "source": [
    "print('- Model Class:')\n",
    "print(model_cls)\n",
    "print('\\n- Configuration:')\n",
    "print(cfg)\n",
    "print('\\n- Tokenizer:')\n",
    "print(tokenizer)\n",
    "print('\\n- Path of the weights:')\n",
    "print(local_params_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a new backbone model in Gluon, you can just use the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (encoder): BertTransformer(\n",
      "    (all_layers): HybridSequential(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (6): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (7): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (8): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (9): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (10): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (11): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_embed): Embedding(28996 -> 768, float32)\n",
      "  (embed_layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "  (embed_dropout): Dropout(p = 0.1, axes=())\n",
      "  (token_type_embed): Embedding(2 -> 768, float32)\n",
      "  (token_pos_embed): PositionalEmbedding(\n",
      "    (_embed): LearnedPositionalEmbedding(units=768, max_length=512, mode=clip, dtype=float32)\n",
      "  )\n",
      "  (pooler): Dense(768 -> 768, Activation(tanh))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "backbone = model_cls.from_cfg(cfg)\n",
    "backbone.hybridize()\n",
    "backbone.load_parameters(local_params_path)\n",
    "print(backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly use the `backbone` to extract contextual embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs= [[  101.   144.  7535.  1320. 20734.  2101.  6618. 16681.  9474. 21239.\n",
      "   2101.  2645.   119.   102.]]\n",
      "Token Types= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Valid Length= [14.]\n"
     ]
    }
   ],
   "source": [
    "text_input = 'GluonNLP helps practitioners solve NLP problems.'\n",
    "token_ids = tokenizer.encode(text_input, int)\n",
    "token_ids = mx.np.array([[tokenizer.vocab.cls_id] + token_ids + [tokenizer.vocab.sep_id]])\n",
    "token_types = mx.np.array([0] * len(token_ids[0]))\n",
    "valid_length = mx.np.array([len(token_ids[0])])\n",
    "print('Token IDs=', token_ids)\n",
    "print('Token Types=', token_types)\n",
    "print('Valid Length=', valid_length)\n",
    "mlm_embeddings, cls_embedding = backbone(token_ids, token_types, valid_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n",
      "[[-0.57272696  0.35954106  0.9992546  -0.970326    0.90788835  0.93265927\n",
      "   0.92036456 -0.99692816 -0.8863071  -0.63977534  0.9243325   0.9911606\n",
      "  -0.9990826  -0.9992712   0.8418865  -0.9091129   0.9655406  -0.5252916\n",
      "  -0.9997559  -0.77614385 -0.6188369  -0.99916065  0.18092567  0.96592814\n",
      "   0.90377444  0.06120971  0.95132345  0.99970615  0.80378264 -0.5750337\n",
      "   0.11013095 -0.9629547   0.9514687  -0.9962427   0.07688154  0.4343017\n",
      "   0.86048555 -0.19628136  0.8895614  -0.959134   -0.56514496 -0.88191223\n",
      "   0.66912913 -0.5302047   0.93865705  0.02508944 -0.02480373 -0.11807826\n",
      "  -0.11206948  0.99931514 -0.85191596  0.74189985 -0.99826777  0.91011924\n",
      "   0.96564406  0.41362172  0.9757109   0.10942441 -0.9995759   0.10298529\n",
      "   0.9245132   0.30454144  0.8265128   0.06407724  0.3205736  -0.38333276\n",
      "  -0.8760772  -0.02513863 -0.45231512  0.17965323 -0.08799271  0.24097772\n",
      "   0.92556524 -0.80420023  0.02304729 -0.8587904   0.01149753 -0.9991972\n",
      "   0.9223567   0.99969673  0.8543037  -0.9981464   0.97383064 -0.17741768\n",
      "  -0.7880351   0.82378864 -0.9997391  -0.9973666  -0.03221367 -0.5191716\n",
      "   0.91199386 -0.95089626  0.7295023  -0.92741704  0.9997473  -0.97661954\n",
      "  -0.1068543   0.23487161  0.94677263 -0.7209574  -0.67021334  0.9378533\n",
      "   0.9997148  -0.9989565   0.999388    0.6132567  -0.909989   -0.8443591\n",
      "   0.7970679   0.08398111  0.9653829  -0.957313   -0.84133595 -0.1023627\n",
      "   0.93772167 -0.9497603   0.9627671   0.9114523  -0.24026646  0.9998326\n",
      "  -0.0367447   0.9775483   0.99126244  0.9552482  -0.89671105 -0.11935861\n",
      "  -0.7747569   0.9279598  -0.60885227 -0.31912634  0.6753708  -0.9724548\n",
      "  -0.9991364   0.99686426 -0.13779674  0.99975026 -0.99489063  0.9964154\n",
      "  -0.99957263 -0.887323   -0.89410466 -0.03095118 -0.9880141  -0.16152011\n",
      "   0.9459581  -0.1902414  -0.9442729  -0.8512647   0.5487621  -0.8666255\n",
      "   0.5442635   0.57300574 -0.8898959   0.7197907   0.99855465  0.9595232\n",
      "   0.978665    0.18876745 -0.968893    0.93298453  0.8947519  -0.99744505\n",
      "   0.88829607 -0.997608    0.99713767  0.92027986  0.84018505 -0.9985763\n",
      "   0.9994192  -0.5994594  -0.22784773  0.378951    0.04255122 -0.9996994\n",
      "   0.30769166  0.2996115   0.76702505  0.9985885  -0.9775155   0.99851936\n",
      "   0.09300661  0.36472857  0.7524127   0.99945056 -0.97908914 -0.9055427\n",
      "  -0.9609452   0.2879854   0.84012264  0.8316972   0.45231175  0.91824776\n",
      "   0.9996107   0.36228216 -0.9955247  -0.29518795  0.89807135 -0.1008367\n",
      "   0.9998276  -0.2615302  -0.99900436 -0.91379446  0.95227134  0.59693664\n",
      "  -0.24240129  0.8506364  -0.8505136  -0.31207472  0.9902752  -0.66077\n",
      "   0.99962157  0.14757703  0.78998756  0.7728694   0.9458277  -0.7602068\n",
      "   0.01685664  0.12259609 -0.797936    0.99887186 -0.9977412  -0.0541344\n",
      "   0.03750559 -0.9803039  -0.9904699   0.8995782   0.11668855 -0.946763\n",
      "  -0.11735844  0.84341204  0.10935204  0.91408366  0.96205044 -0.698117\n",
      "  -0.64734256 -0.99903834 -0.999305   -0.7626674  -0.96993774 -0.04229934\n",
      "   0.5939645  -0.30389673 -0.8150367  -0.99958056  0.8788992   0.9182358\n",
      "  -0.9035602  -0.1234644  -0.8229441  -0.9995801   0.7466551  -0.8930868\n",
      "  -0.99556977  0.9970398  -0.9064314   0.99778813  0.77242357 -0.9780626\n",
      "   0.8432033  -0.9996774  -0.19440125 -0.77647245  0.3089258   0.6802765\n",
      "  -0.85439336  0.16421841  0.96481967 -0.924699   -0.8461075   0.8240172\n",
      "  -0.9994396   0.82637334  0.07906532  0.99513346  0.9311359  -0.2973948\n",
      "   0.9428304   0.9136705  -0.9622532  -0.99852616  0.93148386  0.37516102\n",
      "  -0.97369295 -0.04255914  0.9994681  -0.9996889  -0.74976754 -0.9270464\n",
      "  -0.94911015 -0.99802715  0.39626035 -0.89322114  0.34888774  0.9374336\n",
      "   0.5914061   0.2668162   0.96318513  0.7998019   0.35149148 -0.11377402\n",
      "  -0.01737133 -0.968809   -0.53719187  0.8140087   0.07989298 -0.9997393\n",
      "   0.9990288  -0.93560636  0.6813668   0.9420205  -0.9988272   0.7785316\n",
      "   0.2828161  -0.97337    -0.11837209  0.9992803   0.9474213   0.0131318\n",
      "   0.0797747   0.9477625  -0.33309987  0.73603237 -0.863371   -0.6635679\n",
      "   0.06718487 -0.82035804  0.98897964  0.85185194 -0.93616736  0.9973388\n",
      "  -0.09043428  0.5341103  -0.83051574  0.82020307  0.96558523  0.10587498\n",
      "  -0.40211248 -0.00256677  0.31182316 -0.99162805 -0.06590212 -0.9985318\n",
      "  -0.27642548  0.9741623   0.91175944 -0.7937244   0.787463   -0.02133625\n",
      "   0.89421135 -0.99956805  0.9998216  -0.9516301   0.13902168  0.7045218\n",
      "  -0.95808256 -0.58723754  0.97765607  0.99322283  0.9787538  -0.9413926\n",
      "  -0.7387499   0.6776721   0.8082274  -0.984916   -0.10814246 -0.9998416\n",
      "  -0.78143984  0.99328065  0.9994282   0.05957134 -0.13479854 -0.9991249\n",
      "   0.9068844  -0.9307282  -0.91428035  0.03314274 -0.88678384  0.79658604\n",
      "   0.99953943 -0.7713795   0.83034825  0.0970928  -0.9253566   0.908175\n",
      "   0.8451202   0.99888796 -0.9025786   0.3943546   0.9327675  -0.03548962\n",
      "  -0.80116445  0.32767814  0.99965906 -0.75548416 -0.16957867 -0.9979559\n",
      "  -0.07553191 -0.5078224  -0.47109404 -0.664546   -0.06861646 -0.8821989\n",
      "   0.9767463   0.19279248  0.74240506 -0.36047927  0.9487514  -0.27717727\n",
      "   0.10573374 -0.25717962 -0.3263899   0.45071432  0.28527737  0.9422285\n",
      "  -0.84756887  0.99897534 -0.15158486 -0.9997227  -0.99931186 -0.80172366\n",
      "  -0.9987609   0.74086463 -0.88199395  0.9412981   0.95969963 -0.99952424\n",
      "  -0.99982756 -0.93533075 -0.738651    0.8773795   0.81559306  0.14415489\n",
      "   0.3511639  -0.3443092   0.00497564  0.14527558  0.08568326 -0.8782145\n",
      "  -0.4447602  -0.9996936   0.8752211  -0.999641   -0.8191667   0.9988915\n",
      "  -0.9976393  -0.96282595 -0.8387406  -0.93530273 -0.6780193   0.30985722\n",
      "   0.9410713  -0.54892594 -0.7457415  -0.9988884   0.9478013  -0.93298227\n",
      "  -0.04991941 -0.8241666  -0.9247106   0.99831957  0.952476   -0.38424245\n",
      "   0.01649306 -0.9964318   0.9907366  -0.9569276  -0.89775646 -0.9247596\n",
      "   0.02125848 -0.8272318  -0.999035   -0.0502511   0.99907744  0.8976749\n",
      "   0.96968156  0.19332571 -0.29760692 -0.90897655 -0.0042816  -0.99960995\n",
      "   0.83473635  0.91256535 -0.91123605 -0.88282764  0.94053864  0.9129772\n",
      "  -0.9316302  -0.97913384  0.9150447   0.377061    0.93942606 -0.7293547\n",
      "  -0.1406942   0.32146868  0.0936688  -0.96518356 -0.8982302   0.9815729\n",
      "  -0.9998033   0.91885895  0.9976983   0.99956244 -0.1090142   0.11596218\n",
      "  -0.9875994  -0.72909164 -0.51267624  0.42954248 -0.99957055  0.9993963\n",
      "  -0.9997902   0.60839826 -0.6861007   0.9527688   0.970577   -0.44468042\n",
      "  -0.999479   -0.9991144   0.92603266  0.2424001   0.9565849   0.37306693\n",
      "   0.04508884 -0.70045483 -0.05468931  0.9767509  -0.9479154  -0.62165564\n",
      "  -0.99971426  0.99841046  0.38304242 -0.9922663   0.9951783  -0.9972416\n",
      "   0.9230921   0.9232032   0.85008484  0.9419373  -0.99973786  0.9997794\n",
      "  -0.99924785  0.95030797 -0.9997883  -0.9998327   0.99900603 -0.9755133\n",
      "  -0.8532001  -0.9984478  -0.9986881   0.8411947   0.0185339  -0.4198522\n",
      "   0.94630706 -0.9992054  -0.99247193 -0.62013793 -0.96074253 -0.82724106\n",
      "   0.9981874  -0.7283129   0.9197755  -0.22779004  0.89952374  0.429603\n",
      "   0.9978496   0.7887928  -0.82672685 -0.9133553  -0.9708389   0.97274834\n",
      "  -0.6836231   0.25881845  0.92035747  0.24825771 -0.80262315  0.33647114\n",
      "  -0.98974323  0.63752973  0.23508902  0.9732433   0.94107866  0.7440419\n",
      "  -0.10154936 -0.6059681  -0.33988467 -0.9770012   0.4351704  -0.9978155\n",
      "   0.96999156 -0.9682286  -0.11351415 -0.39552063  0.105707   -0.83054554\n",
      "   0.9980488   0.99322283 -0.7687227  -0.02314142  0.956673   -0.88835555\n",
      "   0.9299527  -0.96686566  0.19430988  0.95444995 -0.63422245  0.9305769\n",
      "  -0.18901838 -0.04173645  0.95631844 -0.9779352  -0.9293273  -0.6567718\n",
      "   0.4033776   0.06261794 -0.91541386 -0.11328954  0.99228233 -0.5314791\n",
      "  -0.9983875   0.96245265 -0.9963331  -0.06686199  0.92072463 -0.8409853\n",
      "   0.9992857  -0.8844141   0.20672493  0.24035521 -0.9981866  -0.99964803\n",
      "  -0.11190501 -0.06669486 -0.968065    0.99983007 -0.3112737   0.8556921\n",
      "  -0.9993838   0.28664166  0.99758947  0.18922119  0.8330079  -0.77940404\n",
      "  -0.9275323  -0.9635693  -0.6961601  -0.06323181  0.91519713 -0.56053686\n",
      "  -0.93419933 -0.8672519   0.99964094 -0.987771   -0.92549324 -0.9761448\n",
      "   0.6836622   0.92782706  0.37882838  0.19283308 -0.93922466  0.9425326\n",
      "  -0.9495543   0.9856323  -0.97390956 -0.97956693  0.9986592   0.8662514\n",
      "  -0.9968027   0.27173936 -0.29459724  0.37608513  0.1743569   0.8748482\n",
      "  -0.2767441  -0.12993276 -0.9353336   0.94360757 -0.90953624 -0.9290893\n",
      "  -0.52934134 -0.185237   -0.6623417   0.97162515  0.91135436  0.9996763\n",
      "  -0.9987125   0.7502396  -0.02291708  0.9948681   0.14809896 -0.50778484\n",
      "   0.9112979   0.9978505  -0.75522727  0.9049624  -0.1694614   0.12140393\n",
      "   0.46867168 -0.1716262   0.99889624 -0.95905644 -0.13638417 -0.914708\n",
      "  -0.9994883   0.9995185   0.02630096  0.9775014   0.24239518  0.8888007\n",
      "  -0.7403128   0.96792567 -0.9841502  -0.8802923  -0.99977934  0.10664327\n",
      "  -0.6977942  -0.9521704  -0.22515205  0.76186574 -0.9972239  -0.93269515\n",
      "  -0.44704482 -0.99977225  0.91877013 -0.99063236 -0.89391136 -0.93384665\n",
      "   0.9996624  -0.2714816  -0.8503754   0.92016125 -0.88835764  0.94245124\n",
      "   0.98126537 -0.5595615   0.30461878 -0.07294607 -0.80642617 -0.99572617\n",
      "  -0.94363236 -0.94225395  0.92432237 -0.9293828  -0.66852677  0.98766387\n",
      "   0.9192177  -0.9973501  -0.9862886   0.99827397  0.30141345  0.96363825\n",
      "  -0.34306696 -0.99908936 -0.9991703   0.04758426  0.04795547  0.96647984\n",
      "  -0.2973336   0.62414455  0.7402113  -0.5601925   0.67313874 -0.6887763\n",
      "  -0.37881908 -0.24800517 -0.0806808   0.99973863 -0.8608241   0.9523948 ]]\n"
     ]
    }
   ],
   "source": [
    "print(cls_embedding.shape)\n",
    "print(cls_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 14, 768)\n",
      "[[[ 0.6234134  -0.07291081 -0.13999614 ... -0.22016764  0.27736056\n",
      "    0.05864484]\n",
      "  [ 0.672965   -0.6641126   0.23682861 ... -0.04360078  0.05051699\n",
      "   -0.04530283]\n",
      "  [ 0.55150485 -0.3550599   0.5144368  ...  0.64902544 -0.00809672\n",
      "    0.20128815]\n",
      "  ...\n",
      "  [ 0.22296664  0.14223897 -0.05181786 ...  0.10244231 -0.605693\n",
      "   -0.03737953]\n",
      "  [ 1.2198194  -0.51079684  0.3234357  ...  0.20491412  0.6366281\n",
      "   -0.59139895]\n",
      "  [ 1.1782554  -0.5468905   0.40015438 ...  0.22755586  0.58317333\n",
      "   -0.5389005 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(mlm_embeddings.shape)\n",
    "print(mlm_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from BERT, GluonNLP has provided other backbone models including the recent models like [XLMR](https://arxiv.org/pdf/1911.02116.pdf), [ALBERT](https://arxiv.org/pdf/1909.11942.pdf), [ELECTRA](https://openreview.net/pdf?id=r1xMH1BtvB), and [MobileBERT](https://arxiv.org/pdf/2004.02984.pdf). We can use `list_backbone_names` to list all the backbones that are supported in GluonNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['google_albert_base_v2',\n",
       " 'google_albert_large_v2',\n",
       " 'google_albert_xlarge_v2',\n",
       " 'google_albert_xxlarge_v2',\n",
       " 'google_en_cased_bert_base',\n",
       " 'google_en_cased_bert_large',\n",
       " 'google_en_cased_bert_wwm_large',\n",
       " 'google_en_uncased_bert_base',\n",
       " 'google_en_uncased_bert_large',\n",
       " 'google_en_uncased_bert_wwm_large',\n",
       " 'google_multi_cased_bert_base',\n",
       " 'google_zh_bert_base',\n",
       " 'gluon_electra_small_owt',\n",
       " 'google_electra_base',\n",
       " 'google_electra_large',\n",
       " 'google_electra_small',\n",
       " 'google_uncased_mobilebert',\n",
       " 'fairseq_roberta_base',\n",
       " 'fairseq_roberta_large',\n",
       " 'fairseq_xlmr_base',\n",
       " 'fairseq_xlmr_large',\n",
       " 'fairseq_bart_base',\n",
       " 'fairseq_bart_large']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gluonnlp.models import list_backbone_names\n",
    "list_backbone_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of the command, let's generate a table that shows the number of params of each backbone model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google_albert_base_v2\n",
      "google_albert_large_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/gluon-nlp/src/gluonnlp/utils/misc.py:292: UserWarning: \"embed_factorized_proj.weight\" is not initialized! The total parameter count will not be correct.\n",
      "  'will not be correct.'.format(k))\n",
      "/home/ubuntu/gluon-nlp/src/gluonnlp/utils/misc.py:292: UserWarning: \"embed_layer_norm.gamma\" is not initialized! The total parameter count will not be correct.\n",
      "  'will not be correct.'.format(k))\n",
      "/home/ubuntu/gluon-nlp/src/gluonnlp/utils/misc.py:292: UserWarning: \"embed_layer_norm.beta\" is not initialized! The total parameter count will not be correct.\n",
      "  'will not be correct.'.format(k))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google_albert_xlarge_v2\n",
      "google_albert_xxlarge_v2\n",
      "google_en_cased_bert_base\n",
      "google_en_cased_bert_large\n",
      "google_en_cased_bert_wwm_large\n",
      "google_en_uncased_bert_base\n",
      "google_en_uncased_bert_large\n",
      "google_en_uncased_bert_wwm_large\n",
      "google_multi_cased_bert_base\n",
      "google_zh_bert_base\n",
      "gluon_electra_small_owt\n",
      "google_electra_base\n",
      "google_electra_large\n",
      "google_electra_small\n",
      "google_uncased_mobilebert\n",
      "fairseq_roberta_base\n",
      "fairseq_roberta_large\n",
      "fairseq_xlmr_base\n"
     ]
    }
   ],
   "source": [
    "from gluonnlp.utils.misc import count_parameters\n",
    "param_num_l = []\n",
    "for name in list_backbone_names():\n",
    "    print(name)\n",
    "    model_cls, cfg, tokenizer, local_params_path, _ = get_backbone(name, load_backbone=False)\n",
    "    model = model_cls.from_cfg(cfg)\n",
    "    model.hybridize()\n",
    "    model.initialize()\n",
    "    total_num_params, fixed_num_params = count_parameters(model.collect_params())\n",
    "    param_num_l.append((name, total_num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Start with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the BERT model first. The architecture of BERT is illustrated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google_en_cased_bert_base'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_cls)\n",
    "print(local_params_path)\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = model_cls.from_cfg(cfg)\n",
    "backbone.hybridize()\n",
    "backbone.load_parameters(local_params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = 'GluonNLP helps practitioners solve NLP problems.'\n",
    "token_ids = mx.np.array([[tokenizer.vocab.cls_id] + tokenizer.encode(text_input, int) + [tokenizer.vocab.sep_id]])\n",
    "token_types = mx.np.array([0] * len(token_ids[0]))\n",
    "valid_length = mx.np.array([len(token_ids[0])])\n",
    "print('Token IDs=', token_ids)\n",
    "print('Token Types=', token_types)\n",
    "print('Valid Length=', valid_length)\n",
    "mlm_embeddings, cls_embedding = backbone(token_ids, token_types, valid_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlm_embeddings.shape)\n",
    "print(mlm_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cls_embedding.shape)\n",
    "print(cls_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of Tokenizer and Vocab in GluonNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_string = \"GluonNLP helps practitioners solve NLP problems.\"\n",
    "print('Original:')\n",
    "print('\\t', original_string)\n",
    "print('To string tokens:')\n",
    "print('\\t', tokenizer.encode(original_string))\n",
    "print('To integer values:')\n",
    "print('\\t', tokenizer.encode(original_string, int))\n",
    "print('Vocabulary of the tokenizer:')\n",
    "print('\\t', tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.vocab['helps'])\n",
    "print(tokenizer.vocab.all_tokens[6618])\n",
    "print(tokenizer.vocab.special_tokens)\n",
    "print(tokenizer.vocab.cls_token)\n",
    "print(tokenizer.vocab.mask_token)\n",
    "print(tokenizer.vocab.unk_token)\n",
    "print(tokenizer.encode('😁 means smile'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Other Backbone Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - [ALBERT](https://arxiv.org/pdf/1909.11942.pdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls, cfg, tokenizer, local_params_path, _ = get_backbone('google_albert_large_v2')\n",
    "backbone = model_cls.from_cfg(cfg)\n",
    "print(cfg)\n",
    "print()\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - [ELECTRA](https://arxiv.org/pdf/2003.10555.pdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls, cfg, tokenizer, local_params_path, _ = get_backbone('google_electra_base')\n",
    "backbone = model_cls.from_cfg(cfg)\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - [MobileBERT](https://arxiv.org/pdf/2004.02984.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls, cfg, tokenizer, local_params_path, _ = get_backbone('google_uncased_mobilebert')\n",
    "backbone = model_cls.from_cfg(cfg)\n",
    "backbone.load_parameters(local_params_path)\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Model for Text Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert Figure to describe how to build the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example-1: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, feature_columns, label_column, tokenizer, max_length=128, use_label=True):\n",
    "    out = []\n",
    "    if isinstance(feature_columns, str):\n",
    "        feature_columns = [feature_columns]\n",
    "    cls_id = tokenizer.vocab.cls_id\n",
    "    sep_id = tokenizer.vocab.sep_id\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # Token IDs =      [CLS]    token_ids1       [SEP]      token_ids2         [SEP]\n",
    "        # Segment IDs =      0         0               0           1                 1\n",
    "        encoded_text_l = [tokenizer.encode(row[col_name], int) for col_name in feature_columns]\n",
    "        trimmed_lengths = get_trimmed_lengths([len(ele) for ele in encoded_text_l],\n",
    "                                              max_length=max_length - len(feature_columns) - 1,\n",
    "                                              do_merge=True)\n",
    "        token_ids = [cls_id] + sum([ele[:length] + [sep_id]\n",
    "                          for length, ele in zip(trimmed_lengths, encoded_text_l)], [])\n",
    "        token_types = [0] + sum([[i % 2] * (length + 1) for i, length in enumerate(trimmed_lengths)], [])\n",
    "        valid_length = len(token_ids)\n",
    "        feature = (token_ids, token_types, valid_length)\n",
    "        if use_label:\n",
    "            label = row[label_column]\n",
    "            out.append((feature, label))\n",
    "        else:\n",
    "            out.append(feature)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPredictionNet(nn.HybridBlock):\n",
    "    def __init__(self, backbone, in_units, out_units):\n",
    "        \"\"\"Construct the TextPrediction Network\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        backbone\n",
    "            The backbone model\n",
    "        in_units\n",
    "            The units of the features extracted by the backbone model\n",
    "        out_units\n",
    "            The number of output units\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.out_proj = nn.Dense(in_units=in_units,\n",
    "                                 units=out_units,\n",
    "                                 flatten=False)\n",
    "\n",
    "    def hybrid_forward(self, F, data, token_types, valid_length):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        F\n",
    "        data\n",
    "            The input data.\n",
    "            The shape is (batch_size, seq_length)\n",
    "        token_types\n",
    "            The type of each token.\n",
    "        valid_length\n",
    "            The valid length of each sample.\n",
    "            Shape is (batch_size,)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out\n",
    "            Shape is (batch_size, units)\n",
    "        \"\"\"\n",
    "        _, pooled_out = self.backbone(data, token_types, valid_length)\n",
    "        out = self.out_proj(pooled_out)\n",
    "        return out\n",
    "\n",
    "    def initialize_with_pretrained_backbone(self, backbone_params_path, ctx=None):\n",
    "        \"\"\"Initialize the network with pretrained backbone\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        backbone_params_path\n",
    "        ctx\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        self.backbone.load_parameters(backbone_params_path, ctx=ctx)\n",
    "        self.out_proj.initialize(ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls, cfg, tokenizer, local_params_path, _ = get_backbone('google_uncased_mobilebert')\n",
    "backbone = model_cls.from_cfg(cfg)\n",
    "\n",
    "net = TextPredictionNet(backbone, backbone.units, 2)\n",
    "net.hybridize()\n",
    "ctx_l = get_mxnet_available_ctx()\n",
    "net.initialize_with_pretrained_backbone(local_params_path, ctx_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet('glue/sst/train.parquet')\n",
    "rng_state = np.random.RandomState(123)\n",
    "train_perm = rng_state.permutation(len(train_df))\n",
    "# Just use 2000 samples for training\n",
    "train_df = train_df.iloc[train_perm[:2000]]\n",
    "valid_df = pd.read_parquet('glue/sst/dev.parquet')\n",
    "train_processed = preprocess_data(train_df,\n",
    "                                  feature_columns=['sentence'],\n",
    "                                  label_column='label',\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  use_label=True)\n",
    "dev_processed = preprocess_data(valid_df,\n",
    "                                feature_columns=['sentence'],\n",
    "                                label_column='label',\n",
    "                                tokenizer=tokenizer,\n",
    "                                use_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing,\n",
    "- Train Sample: `((token_ids, token_types, valid_length), label)`\n",
    "- Valid Sample: `(token_ids, token_types, valid_length)`\n",
    "\n",
    "We construct the batchify function based on this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_processed[0])\n",
    "print(dev_processed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batchify = bf.Group(bf.Group(bf.Pad(), bf.Pad(), bf.Stack()),\n",
    "                          bf.Stack())\n",
    "dev_batchify = bf.Group(bf.Pad(), bf.Pad(), bf.Stack())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write the training loop. We use the Triangular learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size, dataset, batchify_function, net, ctx_l,\n",
    "          num_epochs, lr=1E-4, wd=0.01, max_grad_norm=1.0, warmup_ratio=0.1):\n",
    "    assert batch_size % len(ctx_l) == 0\n",
    "    per_device_batch_size = batch_size // len(ctx_l)\n",
    "    epoch_num_updates = len(dataset) // batch_size\n",
    "    max_update = epoch_num_updates * num_epochs\n",
    "    warmup_steps = int(np.ceil(max_update * warmup_ratio))\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=per_device_batch_size,\n",
    "                            batchify_fn=batchify_function,\n",
    "                            num_workers=4,\n",
    "                            shuffle=True)\n",
    "    dataloader = grouper(repeat(dataloader), len(ctx_l))\n",
    "    lr_scheduler = PolyScheduler(max_update=max_update,\n",
    "                                 base_lr=lr,\n",
    "                                 warmup_begin_lr=0.0,\n",
    "                                 pwr=1,\n",
    "                                 final_lr=0.0,\n",
    "                                 warmup_steps=warmup_steps,\n",
    "                                 warmup_mode='linear')\n",
    "    optimizer_params = {'learning_rate': lr,\n",
    "                        'wd': wd,\n",
    "                        'lr_scheduler': lr_scheduler}\n",
    "    trainer = mx.gluon.Trainer(net.collect_params(),\n",
    "                               'adamw',\n",
    "                               optimizer_params)\n",
    "    params = [p for p in net.collect_params().values() if p.grad_req != 'null']\n",
    "    log_loss = 0\n",
    "    log_gnorm = 0\n",
    "    log_step = 0\n",
    "    log_interval = int(epoch_num_updates * 0.1)\n",
    "    for i in range(max_update):\n",
    "        sample_l = next(dataloader)\n",
    "        loss_l = []\n",
    "        for sample, ctx in zip(sample_l, ctx_l):\n",
    "            (token_ids, token_types, valid_length), label = sample\n",
    "            # Move to the corresponding context\n",
    "            token_ids = mx.np.array(token_ids, ctx=ctx)\n",
    "            token_types = mx.np.array(token_types, ctx=ctx)\n",
    "            valid_length = mx.np.array(valid_length, ctx=ctx)\n",
    "            label = mx.np.array(label, ctx=ctx)\n",
    "            with mx.autograd.record():\n",
    "                scores = net(token_ids, token_types, valid_length)\n",
    "                logits = mx.npx.log_softmax(scores, axis=-1)\n",
    "                loss = - mx.npx.pick(logits, label)\n",
    "                loss_l.append(loss.mean() / len(ctx_l))\n",
    "        for loss in loss_l:\n",
    "            loss.backward()\n",
    "        trainer.allreduce_grads()\n",
    "        # Begin Norm Clipping\n",
    "        total_norm, ratio, is_finite = clip_grad_global_norm(params, max_grad_norm)\n",
    "        trainer.update(1.0)\n",
    "        step_loss = sum([loss.asnumpy() for loss in loss_l])\n",
    "        log_loss += step_loss\n",
    "        log_gnorm += total_norm\n",
    "        log_step += 1\n",
    "        if log_step >= log_interval or i == max_update - 1:\n",
    "            print('[Iter {} / {}] avg nll = {}, avg gradient norm = {}'.format(i + 1, max_update, log_loss / log_step, log_gnorm / log_step))\n",
    "            log_loss = 0\n",
    "            log_gnorm = 0\n",
    "            log_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(32, train_processed, train_batchify, net, ctx_l, 3, lr=1E-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(batch_size, dataset, batchify_function, net, ctx_l):\n",
    "    assert batch_size % len(ctx_l) == 0\n",
    "    per_device_batch_size = batch_size // len(ctx_l)\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=per_device_batch_size,\n",
    "                            batchify_fn=batchify_function,\n",
    "                            shuffle=False)\n",
    "    pred = []\n",
    "    for sample_l in grouper(dataloader, len(ctx_l)):\n",
    "        for sample, ctx in zip(sample_l, ctx_l):\n",
    "            if sample is None:\n",
    "                continue\n",
    "            token_ids, token_types, valid_length = sample\n",
    "            token_ids = mx.np.array(token_ids, ctx=ctx)\n",
    "            token_types = mx.np.array(token_types, ctx=ctx)\n",
    "            valid_length = mx.np.array(valid_length, ctx=ctx)\n",
    "            scores = net(token_ids, token_types, valid_length)\n",
    "            probs = mx.npx.softmax(scores, axis=-1)\n",
    "            pred.append(probs.asnumpy())\n",
    "    pred = np.concatenate(pred, axis=0)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(64, dev_processed, dev_batchify, net, ctx_l)\n",
    "accuracy = (pred.argmax(axis=-1) == valid_df['label']).sum() / len(valid_df)\n",
    "print('Accuracy of the Dev Set=', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
