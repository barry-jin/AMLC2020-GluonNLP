{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Load Dataset + Quickstart of Pretrained Backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will load two datasets using the `nlp_data` command and then try out different backbone models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "import gluonnlp\n",
    "from gluonnlp.utils import set_seed\n",
    "mx.npx.set_np()\n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download two datasets from the [GLUE benchmark](https://gluebenchmark.com/):\n",
    "- The Standford Sentiment Treebank (SST-2)\n",
    "- Semantic Textual Similarity Benchmark (STS-B)\n",
    "\n",
    "We will later show how to train prediction models on these two datasets with GluonNLP.\n",
    "\n",
    "First of all, to download the dataset, let's just use the `nlp_data` command. The downloaded dataset are preprocessed to the [parquet](https://parquet.apache.org/) format that can be loaded by [pandas](https://pandas.pydata.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading glue to \"glue\". Selected tasks = sst\n",
      "Processing sst...\n",
      "\tCompleted!\n",
      "Downloading glue to \"glue\". Selected tasks = sts\n",
      "Processing sts...\n",
      "\tCompleted!\n",
      "\u001b[34msst\u001b[m\u001b[m \u001b[34msts\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!nlp_data prepare_glue --benchmark glue -t sst\n",
    "!nlp_data prepare_glue --benchmark glue -t sts\n",
    "!ls glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sst_train_df = pd.read_parquet('glue/sst/train.parquet')\n",
    "sst_valid_df = pd.read_parquet('glue/sst/dev.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>that 's far too tragic to merit such superfici...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>demonstrates that the director of such hollywo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>of saucy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a depressed fifteen-year-old 's suicidal poetry</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>are more deeply thought through than in most `...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0       hide new secretions from the parental units       0\n",
       "1               contains no wit , only labored gags       0\n",
       "2  that loves its characters and communicates som...      1\n",
       "3  remains utterly satisfied to remain the same t...      0\n",
       "4  on the worst revenge-of-the-nerds clichés the ...      0\n",
       "5  that 's far too tragic to merit such superfici...      0\n",
       "6  demonstrates that the director of such hollywo...      1\n",
       "7                                          of saucy       1\n",
       "8   a depressed fifteen-year-old 's suicidal poetry       0\n",
       "9  are more deeply thought through than in most `...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_train_df = pd.read_parquet('glue/sts/train.parquet')\n",
    "sts_valid_df = pd.read_parquet('glue/sts/dev.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>genre</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Some men are fighting.</td>\n",
       "      <td>Two men are fighting.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A man is smoking.</td>\n",
       "      <td>A man is skating.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The man is playing the piano.</td>\n",
       "      <td>The man is playing the guitar.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A man is playing on a guitar and singing.</td>\n",
       "      <td>A woman is playing an acoustic guitar and sing...</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A person is throwing a cat on to the ceiling.</td>\n",
       "      <td>A person throws a cat on the ceiling.</td>\n",
       "      <td>main-captions</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sentence1  \\\n",
       "0                         A plane is taking off.   \n",
       "1                A man is playing a large flute.   \n",
       "2  A man is spreading shreded cheese on a pizza.   \n",
       "3                   Three men are playing chess.   \n",
       "4                    A man is playing the cello.   \n",
       "5                         Some men are fighting.   \n",
       "6                              A man is smoking.   \n",
       "7                  The man is playing the piano.   \n",
       "8      A man is playing on a guitar and singing.   \n",
       "9  A person is throwing a cat on to the ceiling.   \n",
       "\n",
       "                                           sentence2          genre  score  \n",
       "0                        An air plane is taking off.  main-captions   5.00  \n",
       "1                          A man is playing a flute.  main-captions   3.80  \n",
       "2  A man is spreading shredded cheese on an uncoo...  main-captions   3.80  \n",
       "3                         Two men are playing chess.  main-captions   2.60  \n",
       "4                 A man seated is playing the cello.  main-captions   4.25  \n",
       "5                              Two men are fighting.  main-captions   4.25  \n",
       "6                                  A man is skating.  main-captions   0.50  \n",
       "7                     The man is playing the guitar.  main-captions   1.60  \n",
       "8  A woman is playing an acoustic guitar and sing...  main-captions   2.20  \n",
       "9              A person throws a cat on the ceiling.  main-captions   5.00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart of Pretrained Backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bunch of recent papers, especially [BERT](https://arxiv.org/pdf/1810.04805.pdf), have led a new trend for solving NLP problems:\n",
    "- Pretrain a backbone model on a large corpus,\n",
    "- Finetune the backbone to solve the specific NLP task.\n",
    "\n",
    "GluonNLP provides the interface for using the pretrained backbone models. Here, for quickstart, let's load the BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the backbone models via `get_backbone`. For example, you can run the following command to get the backbone of `google_en_cased_bert_base`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonnlp.models import get_backbone\n",
    "model_name = 'google_en_cased_bert_base'\n",
    "model_cls, cfg, tokenizer, local_params_path, _ = get_backbone(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Model Class:\n",
      "<class 'gluonnlp.models.bert.BertModel'>\n",
      "\n",
      "- Configuration:\n",
      "INITIALIZER:\n",
      "  bias: ['zeros']\n",
      "  embed: ['truncnorm', 0, 0.02]\n",
      "  weight: ['truncnorm', 0, 0.02]\n",
      "MODEL:\n",
      "  activation: gelu\n",
      "  attention_dropout_prob: 0.1\n",
      "  compute_layout: auto\n",
      "  dtype: float32\n",
      "  hidden_dropout_prob: 0.1\n",
      "  hidden_size: 3072\n",
      "  layer_norm_eps: 1e-12\n",
      "  layout: NT\n",
      "  max_length: 512\n",
      "  num_heads: 12\n",
      "  num_layers: 12\n",
      "  num_token_types: 2\n",
      "  pos_embed_type: learned\n",
      "  units: 768\n",
      "  vocab_size: 28996\n",
      "VERSION: 1\n",
      "\n",
      "- Tokenizer:\n",
      "HuggingFaceWordPieceTokenizer(\n",
      "   vocab_file = /Users/zhenghuj/.mxnet/models/nlp/google_en_cased_bert_base/vocab-c1defaaa.json\n",
      "   unk_token = [UNK], sep_token = [SEP], cls_token = [CLS]\n",
      "   pad_token = [PAD], mask_token = [MASK]\n",
      "   clean_text = True, handle_chinese_chars = True\n",
      "   strip_accents = False, lowercase = False\n",
      "   wordpieces_prefix = ##\n",
      "   vocab = Vocab(size=28996, unk_token=\"[UNK]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\", sep_token=\"[SEP]\", mask_token=\"[MASK]\")\n",
      ")\n",
      "\n",
      "- Path of the weights:\n",
      "/Users/zhenghuj/.mxnet/models/nlp/google_en_cased_bert_base/model-c566c289.params\n"
     ]
    }
   ],
   "source": [
    "print('- Model Class:')\n",
    "print(model_cls)\n",
    "print('\\n- Configuration:')\n",
    "print(cfg)\n",
    "print('\\n- Tokenizer:')\n",
    "print(tokenizer)\n",
    "print('\\n- Path of the weights:')\n",
    "print(local_params_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Backbone\n",
    "\n",
    "To create a new backbone model in Gluon, you can just use the following commands:\n",
    "\n",
    "(`backbone.hybridize()` allows computation to be done using the symbolic backend. You can refer to [this](https://mxnet.apache.org/versions/1.6/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html) page on MXNet hybridization.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (encoder): BertTransformer(\n",
      "    (all_layers): HybridSequential(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (6): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (7): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (8): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (9): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (10): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "      (11): TransformerEncoderLayer(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attn_qkv): Dense(768 -> 2304, linear)\n",
      "        (attention_proj): Dense(768 -> 768, linear)\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "           query_units=768,\n",
      "           num_heads=12,\n",
      "           attention_dropout=0.1,\n",
      "           scaled=True,\n",
      "           normalized=False,\n",
      "           layout=\"NTK\",\n",
      "           use_einsum=False,\n",
      "           dtype=float32\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        (ffn): PositionwiseFFN(\n",
      "        \tunits=768,\n",
      "        \thidden_size=3072,\n",
      "        \tactivation_dropout=0.0,\n",
      "        \tactivation=gelu,\n",
      "        \tdropout=0.1,\n",
      "        \tnormalization=layer_norm,\n",
      "        \tlayer_norm_eps=1e-12,\n",
      "        \tpre_norm=False,\n",
      "        \tdtype=float32\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_embed): Embedding(28996 -> 768, float32)\n",
      "  (embed_layer_norm): LayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "  (embed_dropout): Dropout(p = 0.1, axes=())\n",
      "  (token_type_embed): Embedding(2 -> 768, float32)\n",
      "  (token_pos_embed): PositionalEmbedding(\n",
      "    (_embed): LearnedPositionalEmbedding(units=768, max_length=512, mode=clip, dtype=float32)\n",
      "  )\n",
      "  (pooler): Dense(768 -> 768, Activation(tanh))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "backbone = model_cls.from_cfg(cfg)\n",
    "backbone.hybridize()\n",
    "backbone.load_parameters(local_params_path)\n",
    "print(backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly use the `backbone` to extract the embeddings. For BERT, it will output one embedding vector for the whole sentence --- `cls_embedding` and a bounch of contextual embedding vectors for each token --- `token_embeddings`.\n",
    "\n",
    "[INSERT FIGURE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence= hide new secretions from the parental units \n",
      "Tokens= ['hide', 'new', 'secret', '##ions', 'from', 'the', 'parental', 'units']\n",
      "Token IDs= [[  101.  4750.  1207.  3318.  5266.  1121.  1103. 22467.  2338.   102.]]\n",
      "Token Types= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Valid Length= [10.]\n"
     ]
    }
   ],
   "source": [
    "text_input = sst_train_df['sentence'][0]\n",
    "tokens = tokenizer.encode(text_input, str)\n",
    "token_ids = tokenizer.encode(text_input, int)\n",
    "token_ids = mx.np.array([[tokenizer.vocab.cls_id] + token_ids + [tokenizer.vocab.sep_id]])\n",
    "token_types = mx.np.array([0] * len(token_ids[0]))\n",
    "valid_length = mx.np.array([len(token_ids[0])])\n",
    "print('Sentence=', text_input)\n",
    "print('Tokens=', tokens)\n",
    "print('Token IDs=', token_ids)\n",
    "print('Token Types=', token_types)\n",
    "print('Valid Length=', valid_length)\n",
    "token_embeddings, cls_embedding = backbone(token_ids, token_types, valid_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n",
      "[[-0.70912313  0.53099763  0.9998003  -0.9866606   0.94019216  0.6315886\n",
      "   0.9837744  -0.96095276 -0.95775586 -0.60642755  0.9739782   0.99566174\n",
      "  -0.9902677  -0.99969244  0.43570718 -0.96778107  0.97977    -0.52170444\n",
      "  -0.99992543 -0.38844424 -0.2930285  -0.9997452   0.28357014  0.89527607\n",
      "   0.97392505  0.09718111  0.9790452   0.9999052   0.8216679  -0.08883984\n",
      "   0.26693714 -0.98289573  0.68023205 -0.99808586  0.227812    0.10108349\n",
      "   0.24586602 -0.27913767  0.58573306 -0.83144784 -0.5715121  -0.23013894\n",
      "   0.49709597 -0.45805663  0.654542    0.32679302  0.17924467 -0.04300148\n",
      "  -0.19947387  0.99990696 -0.95206505  0.99967813 -0.96945816  0.99622405\n",
      "   0.98914737  0.46140286  0.9910741   0.15409195 -0.992088    0.39813894\n",
      "   0.95724404  0.10991146  0.8809638  -0.15570554  0.33269113 -0.47607285\n",
      "  -0.64785707  0.24575835 -0.52706516  0.3225835   0.48383024  0.22356243\n",
      "   0.96965253 -0.8545227  -0.0252918  -0.8779967   0.08907622 -0.99979186\n",
      "   0.93170214  0.99988896  0.49601823 -0.9991174   0.99054784 -0.35687783\n",
      "  -0.22942457  0.02595922 -0.9879694  -0.99868053  0.1301598  -0.6975652\n",
      "   0.6479567  -0.9797408   0.48520485 -0.64439243  0.9999195  -0.83398616\n",
      "  -0.16350749  0.34621873  0.7815065  -0.35555756 -0.6256356   0.81852055\n",
      "   0.9861365  -0.95692414  0.99028486  0.17341033 -0.92722744 -0.56023633\n",
      "   0.205576    0.13732548  0.9856703  -0.9890133  -0.73785186  0.12957232\n",
      "   0.8658399  -0.8003518   0.9805146   0.71842194 -0.35788894  0.9999255\n",
      "  -0.13492669  0.8086866   0.9962814   0.41495073 -0.30475473 -0.23063137\n",
      "  -0.48100996  0.7361363  -0.32446638 -0.33410972  0.7040149  -0.98624015\n",
      "  -0.98390603  0.9989357  -0.29645613  0.99992025 -0.9983025   0.96804315\n",
      "  -0.99987584 -0.7954263  -0.5509222   0.00173602 -0.9006552   0.3331546\n",
      "   0.98617136  0.0626835  -0.6775683  -0.500653    0.36791104 -0.46138054\n",
      "   0.6064446   0.68072975 -0.9544995   0.9988955   0.9858681   0.82976997\n",
      "   0.90326345  0.17649515 -0.78433204  0.7104871   0.9793454  -0.998678\n",
      "   0.6898534  -0.97399235  0.99894214  0.93739164  0.491253   -0.9814041\n",
      "   0.99976176 -0.13819447 -0.00881219 -0.18429124 -0.13458972 -0.98921126\n",
      "   0.4311681   0.43057296  0.8025709   0.9995556  -0.98920584  0.9997047\n",
      "   0.99279153  0.18337964  0.7244804   0.99094504 -0.9918539  -0.97047716\n",
      "  -0.97837234  0.34525996  0.07042121  0.5316873   0.28127533  0.94442517\n",
      "   0.9848955   0.55633855 -0.9978581  -0.32191858  0.97047484 -0.06837219\n",
      "   0.9999064  -0.29651386 -0.9995534  -0.6966983   0.6940963   0.9551153\n",
      "  -0.22836098  0.95174724 -0.45740658 -0.16556013  0.9410035  -0.9993509\n",
      "   0.96731234  0.23766242  0.35262725  0.8170594   0.9906868  -0.42077184\n",
      "  -0.19564207  0.21587037 -0.57465816  0.9996771  -0.99902815 -0.26663393\n",
      "   0.28968266 -0.9902607  -0.9945611   0.98201895  0.03463604 -0.7213707\n",
      "  -0.25493637 -0.18457296  0.31309584  0.69343877  0.9781911  -0.20347996\n",
      "  -0.00481378 -0.9995792  -0.9663464  -0.71066976 -0.8586005   0.10541617\n",
      "   0.6408008  -0.40886468 -0.88061756 -0.9806821   0.95261985  0.7626148\n",
      "  -0.6342648  -0.18440792 -0.3107097  -0.97628516  0.19034071 -0.50274265\n",
      "  -0.99835557  0.9991186  -0.57489955  0.9768463   0.94731647 -0.9920084\n",
      "   0.5044283  -0.985901   -0.17023103 -0.9990103   0.09609931  0.15818784\n",
      "  -0.3126445   0.01067574  0.98402965 -0.9690242  -0.59319234  0.361562\n",
      "  -0.9997676   0.88259494 -0.11184858  0.9977687   0.02751612 -0.08658528\n",
      "   0.97613406  0.79624254 -0.9820053  -0.9995618   0.64277506  0.9974565\n",
      "  -0.9924495  -0.21130882  0.9997925  -0.9741626  -0.74879974 -0.94033426\n",
      "  -0.9906049  -0.9992529  -0.04490403 -0.5416968   0.22681415  0.9785514\n",
      "   0.29549214  0.02848428  0.99035627  0.9946138   0.1328518  -0.09193362\n",
      "   0.09492457 -0.9673264  -0.9976206   0.40934518  0.183521   -0.9999129\n",
      "   0.9996518  -0.989367    0.99935895  0.7692365  -0.9750184   0.7520885\n",
      "   0.09167933 -0.8396409  -0.0240309   0.99973977  0.97766805 -0.08770706\n",
      "   0.09471997  0.64937276 -0.1514996   0.30610046 -0.280718   -0.25184023\n",
      "   0.24202141 -0.92977554  0.94696516  0.28096983 -0.98477364  0.9747086\n",
      "   0.04741824  0.6460992  -0.39200082  0.85491765  0.99048847 -0.0697453\n",
      "  -0.15210404 -0.25004846 -0.97438926 -0.9099685   0.15399928 -0.98691726\n",
      "  -0.26526985  0.8275368   0.97229755 -0.9772698   0.9964046  -0.23485047\n",
      "   0.6598397  -0.9802499   0.9999465  -0.9973961   0.08398575  0.41545734\n",
      "  -0.5395337   0.02197013  0.9850348   0.93871695  0.9160993  -0.7096047\n",
      "  -0.23356435  0.80399585  0.9424729  -0.9024318  -0.11842213 -0.98959625\n",
      "  -0.03391971  0.99405116  0.98628634 -0.08400218 -0.375371   -0.98401654\n",
      "   0.96780795 -0.54461026 -0.50325704 -0.08069282 -0.69519067  0.56696963\n",
      "   0.98094887 -0.275946    0.57143307  0.16811165 -0.9841504   0.6098048\n",
      "   0.65458906  0.999449   -0.96163726  0.4172111   0.97997844 -0.20373614\n",
      "  -0.6512965   0.48408735  0.9912593  -0.920004   -0.28643847 -0.9990092\n",
      "  -0.11431508 -0.6892625  -0.01464457 -0.24958609  0.05449543 -0.6673372\n",
      "   0.8809226  -0.17344093  0.7497995  -0.10982281  0.95716614 -0.1864695\n",
      "  -0.12268522 -0.3832308  -0.13355823  0.43013152  0.15074715  0.97869563\n",
      "  -0.9575431   0.99980396 -0.04298826 -0.99986565 -0.97093475 -0.5304729\n",
      "  -0.9996938   0.15482453 -0.99585706  0.9824299   0.7551315  -0.9753379\n",
      "  -0.9914058  -0.9972046  -0.99868554  0.40280712  0.51788986 -0.02767378\n",
      "   0.42737174  0.9570443   0.04388636 -0.02946313 -0.19246455 -0.9312641\n",
      "  -0.5006751  -0.9787913   0.4917449  -0.99991494 -0.36954588  0.9908675\n",
      "  -0.9833681  -0.6387664  -0.8858511  -0.6186702  -0.8371623   0.4951994\n",
      "   0.9788494  -0.04468476  0.02384787 -0.99964607  0.98735285 -0.58941853\n",
      "   0.15612535 -0.6592821  -0.9664059   0.99946576  0.73891777 -0.1179148\n",
      "  -0.18034008 -0.99858963  0.9240635  -0.73233354 -0.78813356 -0.97647244\n",
      "   0.12792023 -0.9279718  -0.99970496 -0.03300219  0.96386474  0.9964713\n",
      "   0.96733636  0.38594526 -0.3514428  -0.9588593   0.18783668 -0.9998838\n",
      "   0.51250124  0.6969539  -0.97450703 -0.60203123  0.9911549   0.9654627\n",
      "  -0.631181   -0.9273166   0.6202487   0.37682685  0.9708397  -0.24540615\n",
      "  -0.4695729   0.2862047  -0.12361203 -0.9843397  -0.92410815  0.99113506\n",
      "  -0.986546    0.9797539   0.9650291   0.9882235  -0.12194867  0.04152101\n",
      "  -0.8957931  -0.9975606  -0.6479944   0.2832555  -0.9998468   0.99986184\n",
      "  -0.99989796  0.45102903 -0.20131272  0.7551497   0.98238605 -0.27854306\n",
      "  -0.99977267 -0.9995714   0.11068568 -0.03108909  0.9866784   0.18420677\n",
      "   0.16080098 -0.35277173  0.1351864   0.99359876 -0.5583843  -0.17116958\n",
      "  -0.9720952   0.9992649   0.69940513 -0.9960012   0.9665609  -0.9991914\n",
      "   0.7582752   0.9617448   0.84028673  0.971999   -0.98600656  0.99990547\n",
      "  -0.9997796   0.99781585 -0.99992436 -0.9904515   0.9995296  -0.9891726\n",
      "  -0.20931202 -0.9994523  -0.9827715   0.37248585  0.1802356  -0.5028497\n",
      "   0.98196745 -0.99973005 -0.9966526   0.12603424 -0.7175903  -0.4482951\n",
      "   0.9808356  -0.33139247  0.98844314 -0.06259336  0.9506012   0.03949008\n",
      "   0.984584    0.99862087 -0.63500166 -0.6217123  -0.98388225  0.9807421\n",
      "  -0.1978016   0.31510133  0.9466682  -0.02371383 -0.7071874   0.42388794\n",
      "  -0.99405354  0.2636464  -0.7822577   0.8736043   0.6119008   0.82043064\n",
      "  -0.06348405 -0.4933976  -0.17983913 -0.98904085  0.4498496  -0.99920094\n",
      "   0.9804476  -0.7618979  -0.00218422 -0.3813901   0.32892126 -0.94438195\n",
      "   0.99928355  0.9957442  -0.99952483  0.1381108   0.9766586  -0.5989628\n",
      "   0.9626906  -0.986763   -0.01208432  0.8816148  -0.6924968   0.9698316\n",
      "   0.24679695 -0.09842142  0.97499305 -0.98949784 -0.6312376  -0.6209585\n",
      "   0.29053494  0.16998437 -0.9524797   0.19300888  0.9220837  -0.24068794\n",
      "  -0.99933314  0.5026563  -0.9985959  -0.10911293  0.9524686  -0.02372047\n",
      "   0.99975544 -0.57190335  0.01308782  0.09345754 -0.9993108  -0.99177545\n",
      "   0.14191097 -0.18305087 -0.77246904  0.98824185 -0.13539697  0.34679505\n",
      "  -0.9998639   0.33893284  0.9720028   0.302726    0.79635143 -0.21586722\n",
      "  -0.9564001  -0.836192   -0.58671355 -0.00189637  0.7351252  -0.9377272\n",
      "  -0.2921343  -0.58455074  0.9999166  -0.9929377  -0.9115776  -0.988452\n",
      "   0.11219228  0.6372576   0.45012942  0.1236138  -0.6843907   0.76038367\n",
      "  -0.83024186  0.99136376 -0.9862675  -0.9926844   0.99942565  0.4808648\n",
      "  -0.96751815 -0.12368575 -0.24969041  0.18269455  0.0272303   0.58434194\n",
      "  -0.7905713  -0.15854183 -0.9981454   0.4751401  -0.4371172  -0.98824376\n",
      "  -0.48161116 -0.30607942 -0.9994892   0.98573864  0.95515406  0.9998242\n",
      "  -0.9993335   0.7807244   0.13713287  0.9982674   0.05805503 -0.6261463\n",
      "   0.6714308   0.9994523  -0.29264203  0.553848   -0.05584508  0.06547528\n",
      "   0.07479845 -0.455873    0.98433244 -0.7820918   0.18247338 -0.9704199\n",
      "  -0.9998233   0.9998738  -0.00530519  0.98284584  0.3846702   0.67276704\n",
      "  -0.81670034  0.89701533 -0.90385157 -0.89194626 -0.99992365  0.41347927\n",
      "  -0.9995891  -0.9780134   0.08102934  0.98007154 -0.99915904 -0.9903672\n",
      "  -0.29802015 -0.99992     0.6803773  -0.9541765  -0.39615256 -0.9796596\n",
      "   0.9813396  -0.3468074   0.13625719  0.96203184 -0.9592945   0.8352721\n",
      "   0.7108792   0.7989698   0.2895365   0.18872744 -0.56163335 -0.9718882\n",
      "  -0.6706432  -0.9727423   0.4144956  -0.96833426 -0.61905265  0.9942177\n",
      "   0.97789294 -0.9990125  -0.9910463   0.97953755  0.12185958  0.98464835\n",
      "  -0.5256005  -0.9996094  -0.9996547   0.3027801  -0.24419121  0.9888382\n",
      "  -0.37476197  0.99898016  0.7834061  -0.2613525   0.5787804  -0.1995015\n",
      "  -0.32907233 -0.25714558 -0.20735511  0.9999148  -0.65752673  0.9759005 ]]\n"
     ]
    }
   ],
   "source": [
    "print(cls_embedding.shape)\n",
    "print(cls_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 768)\n",
      "[[[ 0.25098482 -0.20093282 -0.01786667 ... -0.35123447  0.40210432\n",
      "   -0.16942202]\n",
      "  [ 0.2845656  -0.5943626  -0.07495949 ...  0.3651937   0.480888\n",
      "    0.28281704]\n",
      "  [ 0.09450688 -0.06759109  0.40153596 ...  0.3697047   0.46004617\n",
      "    0.04629952]\n",
      "  ...\n",
      "  [ 0.19983983 -0.17685764  0.09971476 ... -0.09942101  0.36892503\n",
      "    0.28015837]\n",
      "  [ 0.11791192 -0.18985072 -0.02815782 ... -0.2251273  -0.14774887\n",
      "   -0.09530129]\n",
      "  [ 0.33991322 -0.0374721  -0.13523811 ... -0.9031352   1.1424122\n",
      "   -0.5947766 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(token_embeddings.shape)\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Backbone Models in GluonNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from BERT, GluonNLP has provided other backbone models including the recent models like [XLMR](https://arxiv.org/pdf/1911.02116.pdf), [ALBERT](https://arxiv.org/pdf/1909.11942.pdf), [ELECTRA](https://openreview.net/pdf?id=r1xMH1BtvB), and [MobileBERT](https://arxiv.org/pdf/2004.02984.pdf). We can use `list_backbone_names` to list all the backbones that are supported in GluonNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['google_albert_base_v2',\n",
       " 'google_albert_large_v2',\n",
       " 'google_albert_xlarge_v2',\n",
       " 'google_albert_xxlarge_v2',\n",
       " 'google_en_cased_bert_base',\n",
       " 'google_en_cased_bert_large',\n",
       " 'google_en_cased_bert_wwm_large',\n",
       " 'google_en_uncased_bert_base',\n",
       " 'google_en_uncased_bert_large',\n",
       " 'google_en_uncased_bert_wwm_large',\n",
       " 'google_multi_cased_bert_base',\n",
       " 'google_zh_bert_base',\n",
       " 'gluon_electra_small_owt',\n",
       " 'google_electra_base',\n",
       " 'google_electra_large',\n",
       " 'google_electra_small',\n",
       " 'gpt2_124M',\n",
       " 'gpt2_355M',\n",
       " 'gpt2_774M',\n",
       " 'google_uncased_mobilebert',\n",
       " 'fairseq_roberta_base',\n",
       " 'fairseq_roberta_large',\n",
       " 'fairseq_xlmr_base',\n",
       " 'fairseq_xlmr_large',\n",
       " 'fairseq_bart_base',\n",
       " 'fairseq_bart_large']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gluonnlp.models import list_backbone_names\n",
    "list_backbone_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of the command, we can compare the number of params of some chosen backbone models:\n",
    "- google_en_uncased_bert_base\n",
    "- google_albert_base_v2\n",
    "- google_uncased_mobilebert\n",
    "- fairseq_bart_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhenghuj/Library/Python/3.8/lib/python/site-packages/mxnet/gluon/block.py:571: UserWarning: Parameter 'weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>#Params (MB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google_en_uncased_bert_base</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google_albert_base_v2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google_uncased_mobilebert</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fairseq_bart_base</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  #Params (MB)\n",
       "0  google_en_uncased_bert_base           109\n",
       "1        google_albert_base_v2            11\n",
       "2    google_uncased_mobilebert            24\n",
       "3            fairseq_bart_base           218"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gluonnlp.utils.misc import count_parameters\n",
    "param_num_l = []\n",
    "for name in ['google_en_uncased_bert_base',\n",
    "             'google_albert_base_v2',\n",
    "             'google_uncased_mobilebert',\n",
    "             'fairseq_bart_base']:\n",
    "    model_cls, cfg, tokenizer, local_params_path, _ = get_backbone(name, load_backbone=False)\n",
    "    model = model_cls.from_cfg(cfg)\n",
    "    model.hybridize()\n",
    "    model.initialize()\n",
    "    total_num_params, fixed_num_params = count_parameters(model.collect_params())\n",
    "    mx.npx.waitall()\n",
    "    param_num_l.append((name, int(total_num_params / 1000000)))\n",
    "df = pd.DataFrame(param_num_l, columns=['Model', '#Params (MB)'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Other Backbones\n",
    "\n",
    "Apart from BERT, it's straightforward to load other backbone models.\n",
    "#### Load ALBERT-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZER:\n",
      "  bias: ['zeros']\n",
      "  embed: ['truncnorm', 0, 0.02]\n",
      "  weight: ['truncnorm', 0, 0.02]\n",
      "MODEL:\n",
      "  activation: gelu(tanh)\n",
      "  attention_dropout_prob: 0.0\n",
      "  compute_layout: auto\n",
      "  dtype: float32\n",
      "  embed_size: 128\n",
      "  hidden_dropout_prob: 0.0\n",
      "  hidden_size: 3072\n",
      "  layer_norm_eps: 1e-12\n",
      "  layout: NT\n",
      "  max_length: 512\n",
      "  num_groups: 1\n",
      "  num_heads: 12\n",
      "  num_layers: 12\n",
      "  num_token_types: 2\n",
      "  pos_embed_type: learned\n",
      "  units: 768\n",
      "  vocab_size: 30000\n",
      "VERSION: 1\n",
      "\n",
      "SentencepieceTokenizer(\n",
      "   model_path = /Users/zhenghuj/.mxnet/models/nlp/google_albert_base_v2/spm-65999e5d.model\n",
      "   lowercase = True, nbest = 0, alpha = 0.0\n",
      "   vocab = Vocab(size=30000, unk_token=\"<unk>\", pad_token=\"<pad>\", cls_token=\"[CLS]\", sep_token=\"[SEP]\", mask_token=\"[MASK]\")\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_cls, cfg, tokenizer, local_params_path, _ = get_backbone('google_albert_base_v2')\n",
    "backbone = model_cls.from_cfg(cfg)\n",
    "backbone.load_parameters(local_params_path)\n",
    "backbone.hybridize()\n",
    "print(cfg)\n",
    "print()\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MobileBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZER:\n",
      "  bias: ['zeros']\n",
      "  embed: ['truncnorm', 0, 0.02]\n",
      "  weight: ['truncnorm', 0, 0.02]\n",
      "MODEL:\n",
      "  activation: relu\n",
      "  attention_dropout_prob: 0.1\n",
      "  bottleneck_strategy: qk_sharing\n",
      "  classifier_activation: False\n",
      "  compute_layout: auto\n",
      "  dtype: float32\n",
      "  embed_size: 128\n",
      "  hidden_dropout_prob: 0.0\n",
      "  hidden_size: 512\n",
      "  inner_size: 128\n",
      "  layer_norm_eps: 1e-12\n",
      "  layout: NT\n",
      "  max_length: 512\n",
      "  normalization: no_norm\n",
      "  num_heads: 4\n",
      "  num_layers: 24\n",
      "  num_stacked_ffn: 4\n",
      "  num_token_types: 2\n",
      "  pos_embed_type: learned\n",
      "  trigram_embed: True\n",
      "  units: 512\n",
      "  use_bottleneck: True\n",
      "  vocab_size: 30522\n",
      "VERSION: 1\n",
      "\n",
      "HuggingFaceWordPieceTokenizer(\n",
      "   vocab_file = /Users/zhenghuj/.mxnet/models/nlp/google_uncased_mobilebert/vocab-e6d2b21d.json\n",
      "   unk_token = [UNK], sep_token = [SEP], cls_token = [CLS]\n",
      "   pad_token = [PAD], mask_token = [MASK]\n",
      "   clean_text = True, handle_chinese_chars = True\n",
      "   strip_accents = False, lowercase = True\n",
      "   wordpieces_prefix = ##\n",
      "   vocab = Vocab(size=30522, unk_token=\"[UNK]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\", sep_token=\"[SEP]\", mask_token=\"[MASK]\")\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_cls, cfg, tokenizer, local_params_path, _ = get_backbone('google_uncased_mobilebert')\n",
    "backbone = model_cls.from_cfg(cfg)\n",
    "backbone.load_parameters(local_params_path)\n",
    "backbone.hybridize()\n",
    "print(cfg)\n",
    "print()\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZER:\n",
      "  bias: ['zeros']\n",
      "  embed: ['xavier', 'gaussian', 'in', 1.0]\n",
      "  weight: ['xavier', 'uniform', 'avg', 1.0]\n",
      "MODEL:\n",
      "  DECODER:\n",
      "    activation: gelu\n",
      "    hidden_size: 3072\n",
      "    num_heads: 12\n",
      "    num_layers: 6\n",
      "    pre_norm: False\n",
      "    recurrent: False\n",
      "    units: 768\n",
      "    use_qkv_bias: True\n",
      "  ENCODER:\n",
      "    activation: gelu\n",
      "    hidden_size: 3072\n",
      "    num_heads: 12\n",
      "    num_layers: 6\n",
      "    pre_norm: False\n",
      "    recurrent: False\n",
      "    units: 768\n",
      "    use_qkv_bias: True\n",
      "  activation_dropout: 0.0\n",
      "  attention_dropout: 0.1\n",
      "  data_norm: True\n",
      "  dropout: 0.1\n",
      "  dtype: float32\n",
      "  layer_norm_eps: 1e-05\n",
      "  layout: NT\n",
      "  max_src_length: 1024\n",
      "  max_tgt_length: 1024\n",
      "  pooler_activation: tanh\n",
      "  pos_embed_type: learned\n",
      "  scale_embed: False\n",
      "  shared_embed: True\n",
      "  tie_weights: True\n",
      "  vocab_size: 51201\n",
      "VERSION: 1\n",
      "\n",
      "HuggingFaceByteBPETokenizer(\n",
      "   merges_file = /Users/zhenghuj/.mxnet/models/nlp/fairseq_bart_base/gpt2-396d4d8e.merges\n",
      "   vocab_file = /Users/zhenghuj/.mxnet/models/nlp/fairseq_bart_base/gpt2-f4dedacb.vocab\n",
      "   add_prefix_space = False, lowercase = False, dropout = None\n",
      "   unicode_normalizer = None, continuing_subword_prefix = None\n",
      "   end_of_word_suffix = None\n",
      "   trim_offsets = False\n",
      "   vocab = Vocab(size=51201, unk_token=\"<unk>\", bos_token=\"<s>\", pad_token=\"<pad>\", eos_token=\"</s>\", mask_token=\"<mask>\")\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_cls, cfg, tokenizer, local_params_path, _ = get_backbone('fairseq_bart_base')\n",
    "backbone = model_cls.from_cfg(cfg)\n",
    "backbone.load_parameters(local_params_path)\n",
    "backbone.hybridize()\n",
    "print(cfg)\n",
    "print()\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence= hide new secretions from the parental units \n",
      "Tokens= ['hide', 'Ġnew', 'Ġsecret', 'ions', 'Ġfrom', 'Ġthe', 'Ġparental', 'Ġunits', 'Ġ']\n",
      "[11.]\n",
      "(1, 11, 51201)\n"
     ]
    }
   ],
   "source": [
    "text_input = sst_train_df['sentence'][0]\n",
    "tokens = tokenizer.encode(text_input, str)\n",
    "token_ids = tokenizer.encode(text_input, int)\n",
    "token_ids = mx.np.array([[tokenizer.vocab.bos_id] + token_ids + [tokenizer.vocab.eos_id]])\n",
    "valid_length = mx.np.array([len(token_ids[0])])\n",
    "token_embeddings = backbone(token_ids, valid_length, token_ids, valid_length)\n",
    "print('Sentence=', text_input)\n",
    "print('Tokens=', tokens)\n",
    "print(valid_length)\n",
    "print(token_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
