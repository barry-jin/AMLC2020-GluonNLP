{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part3: MobileBERT for Text Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have learned 1) the basics about Gluon, 2) how to use the backbone models in GluonNLP, 3) how to use the tokenizer and vocabulary in GluonNLP. In this part, we are going to build a model that finetunes [MobileBERT](https://arxiv.org/pdf/2004.02984.pdf) for text prediction problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonnlp\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from mxnet.gluon import nn\n",
    "from gluonnlp.models import get_backbone\n",
    "from gluonnlp.utils.parameter import clip_grad_global_norm\n",
    "from gluonnlp.utils.preprocessing import get_trimmed_lengths\n",
    "from gluonnlp.utils.misc import get_mxnet_visible_ctx, grouper, repeat\n",
    "from mxnet.gluon.data import batchify as bf\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet.lr_scheduler import PolyScheduler\n",
    "from gluonnlp.utils import set_seed\n",
    "mx.npx.set_np()\n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading glue to \"glue\". Selected tasks = sst\n",
      "Processing sst...\n",
      "Found!\n",
      "Downloading glue to \"glue\". Selected tasks = sts\n",
      "Processing sts...\n",
      "Found!\n"
     ]
    }
   ],
   "source": [
    "!nlp_data prepare_glue --benchmark glue -t sst\n",
    "!nlp_data prepare_glue --benchmark glue -t sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_train_df = pd.read_parquet('glue/sst/train.parquet')\n",
    "# For simplicity, we just use 2000 samples for training for SST\n",
    "rng_state = np.random.RandomState(123)\n",
    "train_perm = rng_state.permutation(len(sst_train_df))\n",
    "# Just use 2000 samples for training\n",
    "sst_train_df = sst_train_df.iloc[train_perm[:2000]]\n",
    "sst_dev_df = pd.read_parquet('glue/sst/dev.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load STS\n",
    "sts_train_df = pd.read_parquet('glue/sts/train.parquet')[['sentence1', 'sentence2', 'score']]\n",
    "sts_dev_df = pd.read_parquet('glue/sts/dev.parquet')[['sentence1', 'sentence2', 'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>goes by quickly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27796</th>\n",
       "      <td>reading lines from a teleprompter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>degraded , handheld blair witch video-cam foot...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12115</th>\n",
       "      <td>reminds us how realistically nuanced a robert ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50834</th>\n",
       "      <td>indulges in the worst elements of all of them .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43622</th>\n",
       "      <td>are nowhere near as vivid as the 19th-century ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3955</th>\n",
       "      <td>throughout a film that is both gripping and co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51011</th>\n",
       "      <td>to see over and over again</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31232</th>\n",
       "      <td>that fails to match the freshness of the actre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32153</th>\n",
       "      <td>this is an undeniably intriguing film from an ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label\n",
       "2434                                    goes by quickly       1\n",
       "27796                 reading lines from a teleprompter       0\n",
       "249    degraded , handheld blair witch video-cam foot...      0\n",
       "12115  reminds us how realistically nuanced a robert ...      1\n",
       "50834   indulges in the worst elements of all of them .       0\n",
       "43622  are nowhere near as vivid as the 19th-century ...      0\n",
       "3955   throughout a film that is both gripping and co...      1\n",
       "51011                        to see over and over again       1\n",
       "31232  that fails to match the freshness of the actre...      0\n",
       "32153  this is an undeniably intriguing film from an ...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Some men are fighting.</td>\n",
       "      <td>Two men are fighting.</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A man is smoking.</td>\n",
       "      <td>A man is skating.</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The man is playing the piano.</td>\n",
       "      <td>The man is playing the guitar.</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A man is playing on a guitar and singing.</td>\n",
       "      <td>A woman is playing an acoustic guitar and sing...</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A person is throwing a cat on to the ceiling.</td>\n",
       "      <td>A person throws a cat on the ceiling.</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sentence1  \\\n",
       "0                         A plane is taking off.   \n",
       "1                A man is playing a large flute.   \n",
       "2  A man is spreading shreded cheese on a pizza.   \n",
       "3                   Three men are playing chess.   \n",
       "4                    A man is playing the cello.   \n",
       "5                         Some men are fighting.   \n",
       "6                              A man is smoking.   \n",
       "7                  The man is playing the piano.   \n",
       "8      A man is playing on a guitar and singing.   \n",
       "9  A person is throwing a cat on to the ceiling.   \n",
       "\n",
       "                                           sentence2  score  \n",
       "0                        An air plane is taking off.   5.00  \n",
       "1                          A man is playing a flute.   3.80  \n",
       "2  A man is spreading shredded cheese on an uncoo...   3.80  \n",
       "3                         Two men are playing chess.   2.60  \n",
       "4                 A man seated is playing the cello.   4.25  \n",
       "5                              Two men are fighting.   4.25  \n",
       "6                                  A man is skating.   0.50  \n",
       "7                     The man is playing the guitar.   1.60  \n",
       "8  A woman is playing an acoustic guitar and sing...   2.20  \n",
       "9              A person throws a cat on the ceiling.   5.00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MobileBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZER:\n",
      "  bias: ['zeros']\n",
      "  embed: ['truncnorm', 0, 0.02]\n",
      "  weight: ['truncnorm', 0, 0.02]\n",
      "MODEL:\n",
      "  activation: relu\n",
      "  attention_dropout_prob: 0.1\n",
      "  bottleneck_strategy: qk_sharing\n",
      "  classifier_activation: False\n",
      "  compute_layout: auto\n",
      "  dtype: float32\n",
      "  embed_size: 128\n",
      "  hidden_dropout_prob: 0.0\n",
      "  hidden_size: 512\n",
      "  inner_size: 128\n",
      "  layer_norm_eps: 1e-12\n",
      "  layout: NT\n",
      "  max_length: 512\n",
      "  normalization: no_norm\n",
      "  num_heads: 4\n",
      "  num_layers: 24\n",
      "  num_stacked_ffn: 4\n",
      "  num_token_types: 2\n",
      "  pos_embed_type: learned\n",
      "  trigram_embed: True\n",
      "  units: 512\n",
      "  use_bottleneck: True\n",
      "  vocab_size: 30522\n",
      "VERSION: 1\n"
     ]
    }
   ],
   "source": [
    "model_cls, cfg, tokenizer, local_params_path, _ = get_backbone('google_uncased_mobilebert')\n",
    "backbone = model_cls.from_cfg(cfg)\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Data\n",
    "\n",
    "First of all, let's preprocess the data. Similar to BERT, in MobileBERT, we warp the data as\n",
    "\n",
    "- <font color='red'>[CLS]</font> TOKENS <font color='red'>[SEP]</font>\n",
    "- <font color='red'>[CLS]</font> TOKENS1 <font color='red'>[SEP]</font> TOKENS2 <font color='red'>[SEP]</font>\n",
    "\n",
    "In addition, to handle a pair of sentences, we generate the segment IDs, which will be 0 for the first sentence and 1 for the second sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, feature_columns, label_column, tokenizer, max_length=128, use_label=True):\n",
    "    out = []\n",
    "    if isinstance(feature_columns, str):\n",
    "        feature_columns = [feature_columns]\n",
    "    cls_id = tokenizer.vocab.cls_id\n",
    "    sep_id = tokenizer.vocab.sep_id\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # Token IDs =      [CLS]    token_ids1       [SEP]      token_ids2         [SEP]\n",
    "        # Segment IDs =      0         0               0           1                 1\n",
    "        encoded_text_l = [tokenizer.encode(row[col_name], int) for col_name in feature_columns]\n",
    "        trimmed_lengths = get_trimmed_lengths([len(ele) for ele in encoded_text_l],\n",
    "                                              max_length=max_length - len(feature_columns) - 1,\n",
    "                                              do_merge=True)\n",
    "        token_ids = [cls_id] + sum([ele[:length] + [sep_id]\n",
    "                          for length, ele in zip(trimmed_lengths, encoded_text_l)], [])\n",
    "        token_types = [0] + sum([[i % 2] * (length + 1) for i, length in enumerate(trimmed_lengths)], [])\n",
    "        valid_length = len(token_ids)\n",
    "        feature = (token_ids, token_types, valid_length)\n",
    "        if use_label:\n",
    "            label = row[label_column]\n",
    "            out.append((feature, label))\n",
    "        else:\n",
    "            out.append(feature)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8526f7603c4373a0a7557c58bd6300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e5fd1ce74c4676a40c3d58396e71a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=872.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                 sentence  label\n",
      "2434                     goes by quickly       1\n",
      "27796  reading lines from a teleprompter       0\n",
      "[(([101, 3632, 2011, 2855, 102], [0, 0, 0, 0, 0], 5), 1), (([101, 3752, 3210, 2013, 1037, 10093, 13699, 21716, 13876, 2121, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 11), 0)]\n"
     ]
    }
   ],
   "source": [
    "processed_sst_train = preprocess_data(sst_train_df, 'sentence', 'label', tokenizer, use_label=True)\n",
    "processed_sst_dev = preprocess_data(sst_dev_df, 'sentence', 'label', tokenizer, use_label=False)\n",
    "print(sst_train_df.iloc[:2])\n",
    "print(processed_sst_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac335ebb7ce442a8597fac6ea7879fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5749.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba62b418edc4b76b733f1199d223c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                         sentence1                    sentence2  score\n",
      "0           A plane is taking off.  An air plane is taking off.    5.0\n",
      "1  A man is playing a large flute.    A man is playing a flute.    3.8\n",
      "[(([101, 1037, 4946, 2003, 2635, 2125, 1012, 102, 2019, 2250, 4946, 2003, 2635, 2125, 1012, 102], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 16), 5.0), (([101, 1037, 2158, 2003, 2652, 1037, 2312, 8928, 1012, 102, 1037, 2158, 2003, 2652, 1037, 8928, 1012, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 18), 3.8)]\n"
     ]
    }
   ],
   "source": [
    "processed_sts_train = preprocess_data(sts_train_df, ['sentence1', 'sentence2'], 'score', tokenizer, use_label=True)\n",
    "processed_sts_dev = preprocess_data(sts_dev_df, ['sentence1', 'sentence2'], 'score', tokenizer, use_label=False)\n",
    "print(sts_train_df.iloc[:2])\n",
    "print(processed_sts_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing,\n",
    "- Train Sample: `((token_ids, token_types, valid_length), label)`\n",
    "- Valid Sample: `(token_ids, token_types, valid_length)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Variable Length Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When sample minibatches for text classification/regression, each text may not have the same length. You can use the built-in batchify functions in Gluon to help convert the data into batches.\n",
    "\n",
    "Recall that we have,\n",
    "- Train Sample: `((token_ids, token_types, valid_length), label)`\n",
    "- Valid Sample: `(token_ids, token_types, valid_length)`\n",
    "\n",
    "The corresponding batchify can be constructed similar to the python typing notation\n",
    "- Train: `Group[Group[Pad, Pad, Stack], Stack]`\n",
    "- Train: `Group[Pad, Pad, Stack]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batchify = bf.Group(bf.Group(bf.Pad(), bf.Pad(), bf.Stack()),\n",
    "                          bf.Stack())\n",
    "dev_batchify = bf.Group(bf.Pad(), bf.Pad(), bf.Stack())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the batchify works. We try to merge two samples into a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[  101,  3632,  2011,  2855,   102,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  3752,  3210,  2013,  1037, 10093, 13699, 21716, 13876,\n",
       "           2121,   102]], dtype=int64),\n",
       "  array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64),\n",
       "  array([ 5, 11], dtype=int64)),\n",
       " array([1, 0], dtype=int64))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batchify([processed_sst_train[0], processed_sst_train[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  101,  2009,  1005,  1055,  1037, 11951,  1998,  2411, 12473,\n",
       "          4990,  1012,   102],\n",
       "        [  101,  4895, 10258,  2378,  8450,  2135, 21657,  1998,  7143,\n",
       "           102,     0,     0]], dtype=int64),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64),\n",
       " array([12, 10], dtype=int64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_batchify([processed_sst_dev[0], processed_sst_dev[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's construct a very simple network that uses the backbone to encode the sentence and adds another fully-connected layer to map the features into scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPredictionNet(nn.HybridBlock):\n",
    "    def __init__(self, backbone, in_units, out_units):\n",
    "        \"\"\"Construct the TextPrediction Network\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        backbone\n",
    "            The backbone model\n",
    "        in_units\n",
    "            The units of the features extracted by the backbone model\n",
    "        out_units\n",
    "            The number of output units\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.out_proj = nn.Dense(in_units=in_units,\n",
    "                                 units=out_units,\n",
    "                                 flatten=False)\n",
    "\n",
    "    def hybrid_forward(self, F, data, token_types, valid_length):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        F\n",
    "        data\n",
    "            The input data.\n",
    "            The shape is (batch_size, seq_length)\n",
    "        token_types\n",
    "            The type of each token.\n",
    "        valid_length\n",
    "            The valid length of each sample.\n",
    "            Shape is (batch_size,)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out\n",
    "            Shape is (batch_size, units)\n",
    "        \"\"\"\n",
    "        _, pooled_out = self.backbone(data, token_types, valid_length)\n",
    "        out = self.out_proj(pooled_out)\n",
    "        return out\n",
    "\n",
    "    def initialize_with_pretrained_backbone(self, backbone_params_path, ctx=None):\n",
    "        \"\"\"Initialize the network with pretrained backbone\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        backbone_params_path\n",
    "        ctx\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        self.backbone.load_parameters(backbone_params_path, ctx=ctx)\n",
    "        self.out_proj.initialize(ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_l = get_mxnet_visible_ctx()\n",
    "net_sst = TextPredictionNet(backbone, backbone.units, 2)\n",
    "net_sst.hybridize()\n",
    "net_sst.initialize_with_pretrained_backbone(local_params_path, ctx_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can write the training loop. Here, we will use the [ADAMW optimizer](https://arxiv.org/pdf/1711.05101.pdf), gradient clipping and [Slanted Triangular Learning Rate](https://arxiv.org/pdf/1801.06146.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Slanted Triangular Learning Rate')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_iterations = 1000\n",
    "warmup = 100\n",
    "x = np.arange(total_iterations)\n",
    "max_lr = 1E-4\n",
    "y = (x < warmup) * max_lr * x / warmup + (x >= warmup) * (total_iterations - x) * max_lr / (total_iterations - warmup)\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title('Slanted Triangular Learning Rate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size, dataset, batchify_function, net, ctx_l,\n",
    "          num_epochs, lr=1E-4, wd=0.01, max_grad_norm=1.0, warmup_ratio=0.1,\n",
    "          loss_function='nll'):\n",
    "    assert batch_size % len(ctx_l) == 0\n",
    "    per_device_batch_size = batch_size // len(ctx_l)\n",
    "    epoch_num_updates = len(dataset) // batch_size\n",
    "    max_update = epoch_num_updates * num_epochs\n",
    "    warmup_steps = int(np.ceil(max_update * warmup_ratio))\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=per_device_batch_size,\n",
    "                            batchify_fn=batchify_function,\n",
    "                            num_workers=4,\n",
    "                            shuffle=True)\n",
    "    dataloader = grouper(repeat(dataloader), len(ctx_l))\n",
    "    \n",
    "    # Construct the learning rate scheduler\n",
    "    lr_scheduler = PolyScheduler(max_update=max_update,\n",
    "                                 base_lr=lr,\n",
    "                                 warmup_begin_lr=0.0,\n",
    "                                 pwr=1,\n",
    "                                 final_lr=0.0,\n",
    "                                 warmup_steps=warmup_steps,\n",
    "                                 warmup_mode='linear')\n",
    "    optimizer_params = {'learning_rate': lr,\n",
    "                        'wd': wd,\n",
    "                        'lr_scheduler': lr_scheduler}\n",
    "    trainer = mx.gluon.Trainer(net.collect_params(),\n",
    "                               'adamw',\n",
    "                               optimizer_params)\n",
    "    params = [p for p in net.collect_params().values() if p.grad_req != 'null']\n",
    "    log_loss = 0\n",
    "    log_gnorm = 0\n",
    "    log_step = 0\n",
    "    log_interval = int(epoch_num_updates * 0.1)\n",
    "    for i in range(max_update):\n",
    "        sample_l = next(dataloader)\n",
    "        loss_l = []\n",
    "        for sample, ctx in zip(sample_l, ctx_l):\n",
    "            (token_ids, token_types, valid_length), label = sample\n",
    "            # Move to the corresponding context\n",
    "            token_ids = mx.np.array(token_ids, ctx=ctx)\n",
    "            token_types = mx.np.array(token_types, ctx=ctx)\n",
    "            valid_length = mx.np.array(valid_length, ctx=ctx)\n",
    "            label = mx.np.array(label, ctx=ctx)\n",
    "            with mx.autograd.record():\n",
    "                scores = net(token_ids, token_types, valid_length)\n",
    "                if loss_function == 'nll':\n",
    "                    logits = mx.npx.log_softmax(scores, axis=-1)\n",
    "                    loss = - mx.npx.pick(logits, label)\n",
    "                elif loss_function == 'mse':\n",
    "                    loss = mx.np.square(scores[:, 0] - label)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "                loss_l.append(loss.mean() / len(ctx_l))\n",
    "        for loss in loss_l:\n",
    "            loss.backward()\n",
    "        trainer.allreduce_grads()\n",
    "        # Begin Norm Clipping\n",
    "        total_norm, ratio, is_finite = clip_grad_global_norm(params, max_grad_norm)\n",
    "        trainer.update(1.0)\n",
    "        step_loss = sum([loss.asnumpy() for loss in loss_l])\n",
    "        log_loss += step_loss\n",
    "        log_gnorm += total_norm\n",
    "        log_step += 1\n",
    "        if log_step >= log_interval or i == max_update - 1:\n",
    "            print('[Iter {} / {}] avg {} = {}, avg gradient norm = {}'.format(i + 1, max_update, loss_function, log_loss / log_step, log_gnorm / log_step))\n",
    "            log_loss = 0\n",
    "            log_gnorm = 0\n",
    "            log_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 6 / 186] avg nll = 464526.3697916667, avg gradient norm = 202609704.0\n",
      "[Iter 12 / 186] avg nll = 673631.84375, avg gradient norm = 271465434.6666667\n",
      "[Iter 18 / 186] avg nll = 283179.0325520833, avg gradient norm = 158474974.66666666\n",
      "[Iter 24 / 186] avg nll = 90273.666015625, avg gradient norm = 69390781.0\n",
      "[Iter 30 / 186] avg nll = 5202.987650553386, avg gradient norm = 6852415.291666667\n",
      "[Iter 36 / 186] avg nll = 352.5524444580078, avg gradient norm = 646393.8177083334\n",
      "[Iter 42 / 186] avg nll = 25.143206199010212, avg gradient norm = 75783.81038411458\n",
      "[Iter 48 / 186] avg nll = 0.9117835064729055, avg gradient norm = 148.4285241762797\n",
      "[Iter 54 / 186] avg nll = 0.8017289042472839, avg gradient norm = 16.799694220225017\n",
      "[Iter 60 / 186] avg nll = 0.715988556543986, avg gradient norm = 15.583660761515299\n",
      "[Iter 66 / 186] avg nll = 0.7277040382226309, avg gradient norm = 18.81360610326131\n",
      "[Iter 72 / 186] avg nll = 0.636403371890386, avg gradient norm = 15.841080665588379\n",
      "[Iter 78 / 186] avg nll = 0.715371310710907, avg gradient norm = 219.40376393000284\n",
      "[Iter 84 / 186] avg nll = 0.47668171922365826, avg gradient norm = 12.000701983769735\n",
      "[Iter 90 / 186] avg nll = 0.4745541165272395, avg gradient norm = 12.144720951716105\n",
      "[Iter 96 / 186] avg nll = 0.37462398161490756, avg gradient norm = 9.801757415135702\n",
      "[Iter 102 / 186] avg nll = 0.4665646056334178, avg gradient norm = 9.087686459223429\n",
      "[Iter 108 / 186] avg nll = 0.401117982963721, avg gradient norm = 9.044080893198648\n",
      "[Iter 114 / 186] avg nll = 0.35817794253428775, avg gradient norm = 6.949978351593018\n",
      "[Iter 120 / 186] avg nll = 0.312560314933459, avg gradient norm = 6.479505022366841\n",
      "[Iter 126 / 186] avg nll = 0.2459396980702877, avg gradient norm = 6.174212614695231\n",
      "[Iter 132 / 186] avg nll = 0.173020638525486, avg gradient norm = 5.835603475570679\n",
      "[Iter 138 / 186] avg nll = 0.18016168288886547, avg gradient norm = 6.322053114573161\n",
      "[Iter 144 / 186] avg nll = 0.20271660387516022, avg gradient norm = 5.371056318283081\n",
      "[Iter 150 / 186] avg nll = 0.1879848800599575, avg gradient norm = 5.813435196876526\n",
      "[Iter 156 / 186] avg nll = 0.18410529009997845, avg gradient norm = 8.458973368008932\n",
      "[Iter 162 / 186] avg nll = 0.3322364538908005, avg gradient norm = 8.594975233078003\n",
      "[Iter 168 / 186] avg nll = 0.19316989928483963, avg gradient norm = 6.831129352251689\n",
      "[Iter 174 / 186] avg nll = 0.25217843676606816, avg gradient norm = 6.440486391385396\n",
      "[Iter 180 / 186] avg nll = 0.19320117433865866, avg gradient norm = 6.325243194897969\n",
      "[Iter 186 / 186] avg nll = 0.17055727293094, avg gradient norm = 4.936547636985779\n"
     ]
    }
   ],
   "source": [
    "train(32, processed_sst_train, train_batchify, net_sst, ctx_l, 3, lr=1E-4, loss_function='nll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(batch_size, dataset, batchify_function, net, ctx_l, classification=True):\n",
    "    per_device_batch_size = (batch_size + len(ctx_l) - 1) // len(ctx_l)\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=per_device_batch_size,\n",
    "                            batchify_fn=batchify_function,\n",
    "                            shuffle=False)\n",
    "    pred = []\n",
    "    for sample_l in grouper(dataloader, len(ctx_l)):\n",
    "        for sample, ctx in zip(sample_l, ctx_l):\n",
    "            if sample is None:\n",
    "                continue\n",
    "            token_ids, token_types, valid_length = sample\n",
    "            token_ids = mx.np.array(token_ids, ctx=ctx)\n",
    "            token_types = mx.np.array(token_types, ctx=ctx)\n",
    "            valid_length = mx.np.array(valid_length, ctx=ctx)\n",
    "            scores = net(token_ids, token_types, valid_length)\n",
    "            if classification:\n",
    "                probs = mx.npx.softmax(scores, axis=-1)\n",
    "                pred.append(probs.asnumpy())\n",
    "            else:\n",
    "                pred.append(scores.asnumpy())\n",
    "    pred = np.concatenate(pred, axis=0)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Dev Set= 0.8589449541284404\n",
      "Sentence= it 's a charming and often affecting journey .  Prediction= [6.5858556e-05 9.9993414e-01] Ground Truth= 1\n",
      "Sentence= unflinchingly bleak and desperate  Prediction= [9.9913019e-01 8.6982024e-04] Ground Truth= 0\n"
     ]
    }
   ],
   "source": [
    "pred = predict(64, processed_sst_dev, dev_batchify, net_sst, ctx_l)\n",
    "accuracy = (pred.argmax(axis=-1) == sst_dev_df['label']).sum() / len(sst_dev_df)\n",
    "print('Accuracy of the Dev Set=', accuracy)\n",
    "print('Sentence=', sst_dev_df['sentence'][0], 'Prediction=', pred[0], 'Ground Truth=', sst_dev_df['label'][0])\n",
    "print('Sentence=', sst_dev_df['sentence'][1], 'Prediction=', pred[1], 'Ground Truth=', sst_dev_df['label'][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for Sentence Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 17 / 358] avg mse = 527478388366559.94, avg gradient norm = 1.808853337152669e+16\n",
      "[Iter 34 / 358] avg mse = 61906266468436.7, avg gradient norm = 2624602135697528.5\n",
      "[Iter 51 / 358] avg mse = 37700690.93477825, avg gradient norm = 6269371405.852984\n",
      "[Iter 68 / 358] avg mse = 3.04906904202304, avg gradient norm = 160.7111073662253\n",
      "[Iter 85 / 358] avg mse = 1.6890815805556612, avg gradient norm = 74.75011926538804\n",
      "[Iter 102 / 358] avg mse = 1.129450557181529, avg gradient norm = 48.2878838707419\n",
      "[Iter 119 / 358] avg mse = 1.3591939800500275, avg gradient norm = 88.09840954051299\n",
      "[Iter 136 / 358] avg mse = 1.0875282518228744, avg gradient norm = 48.7095625260297\n",
      "[Iter 153 / 358] avg mse = 0.9594265655031452, avg gradient norm = 44.05969485114603\n",
      "[Iter 170 / 358] avg mse = 0.8630860586873328, avg gradient norm = 30.31046182969037\n",
      "[Iter 187 / 358] avg mse = 0.6990983851593626, avg gradient norm = 27.60971809835995\n",
      "[Iter 204 / 358] avg mse = 0.7467553276688913, avg gradient norm = 40.243509124307074\n",
      "[Iter 221 / 358] avg mse = 0.7125168625150694, avg gradient norm = 26.696138381958008\n",
      "[Iter 238 / 358] avg mse = 0.6261176826997786, avg gradient norm = 26.257740020751953\n",
      "[Iter 255 / 358] avg mse = 0.6388663163073073, avg gradient norm = 22.547951305613797\n",
      "[Iter 272 / 358] avg mse = 0.6894904240457643, avg gradient norm = 28.30427775663488\n",
      "[Iter 289 / 358] avg mse = 0.5666439681362753, avg gradient norm = 23.00908557106467\n",
      "[Iter 306 / 358] avg mse = 0.6518481901331981, avg gradient norm = 19.319913190953873\n",
      "[Iter 323 / 358] avg mse = 0.5739929884615027, avg gradient norm = 23.181817840127383\n",
      "[Iter 340 / 358] avg mse = 0.5283260942122026, avg gradient norm = 18.315171185661764\n",
      "[Iter 357 / 358] avg mse = 0.5512983024804875, avg gradient norm = 19.45353883855483\n",
      "[Iter 358 / 358] avg mse = 0.6272301526064398, avg gradient norm = 12.875463485717773\n"
     ]
    }
   ],
   "source": [
    "net_sts = TextPredictionNet(backbone, backbone.units, 1)\n",
    "net_sts.hybridize()\n",
    "net_sts.initialize_with_pretrained_backbone(local_params_path, ctx_l)\n",
    "train(32, processed_sts_train, train_batchify, net_sts, ctx_l, 2, lr=5E-5, loss_function='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.5490487312676965\n"
     ]
    }
   ],
   "source": [
    "pred = predict(64, processed_sts_dev, dev_batchify, net_sts, ctx_l, classification=False)\n",
    "mse = np.square((pred[:, 0] - sts_dev_df['score'])).mean()\n",
    "print('MSE =', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870da4d7a2fb4f93b9d52ee740cad545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4.0387745] [3.8400981] [1.1482639]\n"
     ]
    }
   ],
   "source": [
    "sentences = ['The child is riding a horse.',\n",
    "             'The young boy is riding a horse.',\n",
    "             'The young man is riding a horse.',\n",
    "             'The young man is riding a bicycle.']\n",
    "\n",
    "pred_dataset_df = pd.DataFrame([(sentences[0], sentences[1]),\n",
    "                                (sentences[0], sentences[2]),\n",
    "                                (sentences[0], sentences[3])],\n",
    "                               columns=['sentence1', 'sentence2'])\n",
    "pred_dataset_processed = preprocess_data(pred_dataset_df, ['sentence1', 'sentence2'],\n",
    "                                         'score', tokenizer, use_label=False)\n",
    "pred = predict(64, pred_dataset_processed, dev_batchify, net_sts, ctx_l, classification=False)\n",
    "print(pred[0], pred[1], pred[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
