{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2: Basic Usage of Tokenizer and Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will understand the basic usage of the tokenizer and vocabulary class in GluonNLP. The combination of tokenizer and vocabulary helps convert the raw text string to a sequence of integer values that can be fed into deep networks.\n",
    "\n",
    "The usual workflow will be:\n",
    "\n",
    "```\n",
    "raw text => normalized (cleaned) text => tokens => network\n",
    "```\n",
    "\n",
    "In addition, in GluonNLP, we provide the `encode_with_offsets` option, in which you can get the character-level start and end position of the encoded tokens. This helps you solve span extraction tasks that appear also in Quesiton Answering, which will be illustrated in the next tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonnlp\n",
    "from gluonnlp.models import get_backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization converts the raw sentence into a series of tokens. For example, let's consider two basic tokenizers, the `WhitespaceTokenizer` and the `MosesTokenizer`. We can simply call `tokenizer.encode()` to encode the sequence to a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:\n",
      "\"#COVID19 cases in Sunnyvale declined over the last 7 days.\"\n",
      "\n",
      "Output of WhitespaceTokenizer:\n",
      "['\"#COVID19', 'cases', 'in', 'Sunnyvale', 'declined', 'over', 'the', 'last', '7', 'days.\"']\n",
      "Output of MosesTokenizer:\n",
      "['&quot;', '#', 'COVID19', 'cases', 'in', 'Sunnyvale', 'declined', 'over', 'the', 'last', '7', 'days', '.', '&quot;']\n"
     ]
    }
   ],
   "source": [
    "from gluonnlp.data.tokenizers import WhitespaceTokenizer, MosesTokenizer\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "moses_tokenizer = MosesTokenizer('en')\n",
    "\n",
    "sentence = '\"#COVID19 cases in Sunnyvale declined over the last 7 days.\"'\n",
    "\n",
    "print('Original Sentence:')\n",
    "print(sentence + '\\n')\n",
    "\n",
    "print('Output of WhitespaceTokenizer:')\n",
    "print(whitespace_tokenizer.encode(sentence))\n",
    "\n",
    "print('Output of MosesTokenizer:')\n",
    "print(moses_tokenizer.encode(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the advanced tokenizers will deal with the punctuations automatically. To merge back a list of tokens to the original sentence, we can use `tokenizer.decode()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence= \"#COVID19 cases in Sunnyvale declined over the last 7 days.\"\n",
      "Tokens= ['&quot;', '#', 'COVID19', 'cases', 'in', 'Sunnyvale', 'declined', 'over', 'the', 'last', '7', 'days', '.', '&quot;']\n",
      "Decoded Sentence= \"# COVID19 cases in Sunnyvale declined over the last 7 days.\"\n"
     ]
    }
   ],
   "source": [
    "tokens = moses_tokenizer.encode(sentence)\n",
    "recovered_sentence = moses_tokenizer.decode(tokens)\n",
    "print('Original Sentence=', sentence)\n",
    "print('Tokens=', tokens)\n",
    "print('Decoded Sentence=', recovered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the tokenization phase, we can create a vocabulary object that maps string token to the integers, which can then be wrapped as the input to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=14, unk_token=\"<unk>\")\n",
      "['&quot;', '#', 'COVID19', 'cases', 'in', 'Sunnyvale', 'declined', 'over', 'the', 'last', '7', 'days', '.', '<unk>']\n",
      "['&quot;', '#', 'COVID19', 'cases', 'in', 'Sunnyvale', 'declined', 'over', 'the', 'last', '7', 'days', '.']\n",
      "['<unk>']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from gluonnlp.data.vocab import Vocab\n",
    "non_duplicate_tokens = list(Counter(tokens))\n",
    "vocab = Vocab(non_duplicate_tokens)\n",
    "print(vocab)\n",
    "print(vocab.all_tokens)\n",
    "print(vocab.non_special_tokens)\n",
    "print(vocab.special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the vocabulary to convert string tokens to integers. The order will be the same as in the `vocab.all_tokens`. Also, the `unk_token` is a special token that is used to handle unseen inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&quot; --> 0\n",
      "# --> 1\n",
      "COVID19 --> 2\n",
      "cases --> 3\n",
      "in --> 4\n",
      "Sunnyvale --> 5\n",
      "declined --> 6\n",
      "over --> 7\n",
      "the --> 8\n",
      "last --> 9\n",
      "7 --> 10\n",
      "days --> 11\n",
      ". --> 12\n",
      "&quot; --> 0\n",
      "\n",
      "unk_token = <unk> , unk_id = 13\n",
      "ü§£ --> 13\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print('{} --> {}'.format(token, vocab[token]))\n",
    "print()\n",
    "print('unk_token =', vocab.unk_token, ', unk_id =', vocab.unk_id)\n",
    "print('ü§£ --> {}'.format(vocab['ü§£']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can attach the vocabulary to the MosesTokenizer and use `encode(..., int)` method to directly map the sentence to a sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 0]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moses_tokenizer.set_vocab(vocab)\n",
    "moses_tokenizer.encode(sentence, int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of **Subword Tokenization** is widely adopted in state-of-the-art pretrained models. For example, BERT used the [WordPiece](https://arxiv.org/pdf/1609.08144.pdf) subword tokenization algorithm. Before explaining the meaning of **subword**, let's first load the tokenizer of the BERT-cased model and see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"',\n",
       " '#',\n",
       " 'CO',\n",
       " '##VI',\n",
       " '##D',\n",
       " '##19',\n",
       " 'cases',\n",
       " 'in',\n",
       " 'Sunny',\n",
       " '##vale',\n",
       " 'declined',\n",
       " 'over',\n",
       " 'the',\n",
       " 'last',\n",
       " '7',\n",
       " 'days',\n",
       " '.',\n",
       " '\"']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, tokenizer, _, _ = get_backbone('google_en_cased_bert_base')\n",
    "tokenizer.encode(sentence, str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with the results of MosesTokenizer, we can find that COVID19 is converted to **CO, ##VI, ##D, ##19**, and Sunnyvale is converted to **Sunny ##vale**.\n",
    "\n",
    "This helps you compress the vocabulary size because there are lots of words with shared prefix/postfix. For example, \"dog\", \"dogs\", and \"dogcatcher\"; \"boyfriend\", \"girlfriend\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access to the vocabulary of the WordPiece tokenizer used in BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab(size=28996, unk_token=\"[UNK]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\", sep_token=\"[SEP]\", mask_token=\"[MASK]\")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In BERT, there will be the special **[CLS]**, **[SEP]** tokens. We can fetch the id and value of these tokens via `vocab.cls_token`, `vocab.cls_id`, and `vocab.sep_token`, `vocab.sep_id`. Also, there is the unknow token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token =  [CLS] , cls_id =  101\n",
      "sep_token =  [SEP] , sep_id =  102\n",
      "['[UNK]', 'means', 'smile']\n"
     ]
    }
   ],
   "source": [
    "print('cls_token = ', tokenizer.vocab.cls_token, ', cls_id = ', tokenizer.vocab.cls_id)\n",
    "print('sep_token = ', tokenizer.vocab.sep_token, ', sep_id = ', tokenizer.vocab.sep_id)\n",
    "print(tokenizer.encode('üòÅ means smile'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for the input to the BERT model, we will need to append the CLS token to the beginning and the SEP token to the end. Thus, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 107, 108, 18732, 23314, 2137, 16382, 2740, 1107, 17321, 18236, 5799, 1166, 1103, 1314, 128, 1552, 119, 107, 102]\n"
     ]
    }
   ],
   "source": [
    "bert_token_input = [tokenizer.vocab.cls_id] + tokenizer.encode(sentence, int) + [tokenizer.vocab.sep_id]\n",
    "print(bert_token_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GluonNLP, to better facilitate span extraction applications, the tokenizers support the `encode_with_offset` functionality, which also returns the character-level offsets of the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', '#', 'CO', '##VI', '##D', '##19', 'cases', 'in', 'Sunny', '##vale', 'declined', 'over', 'the', 'last', '7', 'days', '.', '\"'] [(0, 1), (1, 2), (2, 4), (4, 6), (6, 7), (7, 9), (10, 15), (16, 18), (19, 24), (24, 28), (29, 37), (38, 42), (43, 46), (47, 51), (52, 53), (54, 58), (58, 59), (59, 60)]\n"
     ]
    }
   ],
   "source": [
    "encoded_tokens, offsets = tokenizer.encode_with_offsets(sentence, str)\n",
    "print(encoded_tokens, offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token = \", sentence[0:1] = \"\n",
      "token = #, sentence[1:2] = #\n",
      "token = CO, sentence[2:4] = CO\n",
      "token = ##VI, sentence[4:6] = VI\n",
      "token = ##D, sentence[6:7] = D\n",
      "token = ##19, sentence[7:9] = 19\n",
      "token = cases, sentence[10:15] = cases\n",
      "token = in, sentence[16:18] = in\n",
      "token = Sunny, sentence[19:24] = Sunny\n",
      "token = ##vale, sentence[24:28] = vale\n",
      "token = declined, sentence[29:37] = declined\n",
      "token = over, sentence[38:42] = over\n",
      "token = the, sentence[43:46] = the\n",
      "token = last, sentence[47:51] = last\n",
      "token = 7, sentence[52:53] = 7\n",
      "token = days, sentence[54:58] = days\n",
      "token = ., sentence[58:59] = .\n",
      "token = \", sentence[59:60] = \"\n"
     ]
    }
   ],
   "source": [
    "for token, offset in zip(encoded_tokens, offsets):\n",
    "    print('token = {}, sentence[{}:{}] = {}'.format(token, offset[0], offset[1], sentence[offset[0]:offset[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also support to directly map the sentence to a list of integers by using `encode_with_offsets(sentence, int)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([107,\n",
       "  108,\n",
       "  18732,\n",
       "  23314,\n",
       "  2137,\n",
       "  16382,\n",
       "  2740,\n",
       "  1107,\n",
       "  17321,\n",
       "  18236,\n",
       "  5799,\n",
       "  1166,\n",
       "  1103,\n",
       "  1314,\n",
       "  128,\n",
       "  1552,\n",
       "  119,\n",
       "  107],\n",
       " [(0, 1),\n",
       "  (1, 2),\n",
       "  (2, 4),\n",
       "  (4, 6),\n",
       "  (6, 7),\n",
       "  (7, 9),\n",
       "  (10, 15),\n",
       "  (16, 18),\n",
       "  (19, 24),\n",
       "  (24, 28),\n",
       "  (29, 37),\n",
       "  (38, 42),\n",
       "  (43, 46),\n",
       "  (47, 51),\n",
       "  (52, 53),\n",
       "  (54, 58),\n",
       "  (58, 59),\n",
       "  (59, 60)])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_with_offsets(sentence, int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
